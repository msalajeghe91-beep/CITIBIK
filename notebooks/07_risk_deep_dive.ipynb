{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8edcd816",
   "metadata": {},
   "source": [
    "# Risk Deep Dive: Geographic & Temporal Analysis\n",
    "   \n",
    "   Detailed risk analysis for AXA Deutschland bike-share insurance pricing.\n",
    "   \n",
    "   **Prerequisites**: Run `make scorecard MODE=nyc` first.\n",
    "   \n",
    "   **Audience**: Technical teams, actuaries, data scientists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1edc51",
   "metadata": {},
   "source": [
    "## How to run (recommended)\n",
    "\n",
    "From the repo root:\n",
    "\n",
    "```bash\n",
    "make all MODE=nyc YEARS=\"2023 2024\" MONTHS=\"1 2 3\"\n",
    "make report MODE=nyc YEARS=\"2023 2024\" MONTHS=\"1 2 3\"\n",
    "```\n",
    "\n",
    "The Makefile sets environment variables (e.g. `CITIBIKE_PARQUET_DIR`, `CITIBIKE_YEARS`, `CITIBIKE_MONTHS`) which this notebook reads.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b53285",
   "metadata": {},
   "source": [
    "## Analysis Overview\n",
    "   \n",
    "   This notebook provides detailed risk analysis including:\n",
    "   1. **High-Risk Station Identification** - Top 10% riskiest stations\n",
    "   2. **Geographic Clustering** - Do high-risk stations cluster?\n",
    "   3. **Risk Distribution** - How is risk distributed across the system?\n",
    "   4. **Temporal Patterns** - When is exposure highest?\n",
    "   5. **Executive Summary** - AXA-ready metrics\n",
    "   \n",
    "   **Run time**: ~2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae9f74bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display, Markdown\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# --- Setup (STRICT): locate repo, resolve RUN_DIR + PARQUET_DIR, load summaries safely ---\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    from IPython.display import display, Markdown\n",
    "    _HAVE_IPYTHON = True\n",
    "except Exception:\n",
    "    display = print\n",
    "    Markdown = lambda x: x\n",
    "    _HAVE_IPYTHON = False\n",
    "\n",
    "# Ensure figures render into the executed notebook outputs\n",
    "try:\n",
    "    get_ipython().run_line_magic(\"matplotlib\", \"inline\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "plt.ioff()  # nbconvert-friendly (prevents interactive windows)\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    start = start.resolve()\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"Makefile\").exists():\n",
    "            return p\n",
    "    return start\n",
    "\n",
    "REPO_ROOT = find_repo_root(Path.cwd())\n",
    "SUMMARIES_ROOT = REPO_ROOT / \"summaries\"\n",
    "\n",
    "def _parse_int_list(val: str | None):\n",
    "    if val is None:\n",
    "        return None\n",
    "    s = str(val).strip()\n",
    "    if not s:\n",
    "        return None\n",
    "    parts = re.split(r\"[,\\s]+\", s)\n",
    "    out = []\n",
    "    for p in parts:\n",
    "        if not p:\n",
    "            continue\n",
    "        try:\n",
    "            out.append(int(p))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return out or None\n",
    "\n",
    "# Inputs from Makefile (recommended)\n",
    "PARQUET_DIR_ENV = (os.environ.get(\"CITIBIKE_PARQUET_DIR\") or \"\").strip()\n",
    "RUN_DIR_ENV     = (os.environ.get(\"CITIBIKE_RUN_DIR\") or \"\").strip()\n",
    "MODE_ENV        = (os.environ.get(\"CITIBIKE_MODE\") or os.environ.get(\"MODE\") or \"\").strip().lower()\n",
    "\n",
    "YEARS_FILTER  = _parse_int_list(os.environ.get(\"CITIBIKE_YEARS\")  or os.environ.get(\"YEARS\"))\n",
    "MONTHS_FILTER = _parse_int_list(os.environ.get(\"CITIBIKE_MONTHS\") or os.environ.get(\"MONTHS\"))\n",
    "\n",
    "PARQUET_DIR = Path(PARQUET_DIR_ENV) if PARQUET_DIR_ENV else Path()\n",
    "if RUN_DIR_ENV:\n",
    "    RUN_DIR = Path(RUN_DIR_ENV)\n",
    "else:\n",
    "    # Infer RUN_TAG from parquet folder name; expects .../citibike_parquet/<RUN_TAG>\n",
    "    run_tag = PARQUET_DIR.name if str(PARQUET_DIR).strip() else \"\"\n",
    "    RUN_DIR = SUMMARIES_ROOT / run_tag if run_tag else Path()\n",
    "\n",
    "# Resolve relative -> absolute (relative to REPO_ROOT)\n",
    "if RUN_DIR and (not RUN_DIR.is_absolute()):\n",
    "    RUN_DIR = (REPO_ROOT / RUN_DIR).resolve()\n",
    "if PARQUET_DIR and (not PARQUET_DIR.is_absolute()):\n",
    "    PARQUET_DIR = (REPO_ROOT / PARQUET_DIR).resolve()\n",
    "\n",
    "# ---- STRICT checks ----\n",
    "if not (REPO_ROOT / \"Makefile\").exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Expected to find repo root (Makefile) by walking up from CWD.\\n\"\n",
    "        f\"  CWD: {Path.cwd().resolve()}\\n\"\n",
    "        f\"  Derived REPO_ROOT: {REPO_ROOT}\"\n",
    "    )\n",
    "\n",
    "if not SUMMARIES_ROOT.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Expected summaries/ folder not found at: {SUMMARIES_ROOT}\\n\"\n",
    "        \"Run: make summarize (or make all) first.\"\n",
    "    )\n",
    "\n",
    "if not RUN_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Expected summaries run_dir not found:\\n  {RUN_DIR}\\n\"\n",
    "        \"Run: make summarize (or make all) first.\"\n",
    "    )\n",
    "\n",
    "if not PARQUET_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Expected parquet dir not found:\\n  {PARQUET_DIR}\\n\"\n",
    "        \"Run: make ingest (or make all) first.\"\n",
    "    )\n",
    "\n",
    "REQUIRED_RUN_FILES = [\n",
    "    \"citibike_trips_by_year.csv\",\n",
    "    \"citibike_trips_by_month.csv\",\n",
    "    \"citibike_trips_by_dow.csv\",\n",
    "    \"citibike_trips_by_hour.csv\",\n",
    "]\n",
    "missing = [f for f in REQUIRED_RUN_FILES if not (RUN_DIR / f).exists()]\n",
    "if missing:\n",
    "    raise FileNotFoundError(\n",
    "        \"Missing required summary CSVs in run_dir:\\n\"\n",
    "        f\"  {RUN_DIR}\\n\"\n",
    "        f\"Missing: {missing}\\n\"\n",
    "        \"Run: make summarize (or make all) first.\"\n",
    "    )\n",
    "\n",
    "print(\"REPO_ROOT:\", REPO_ROOT)\n",
    "print(\"PARQUET_DIR:\", PARQUET_DIR)\n",
    "print(\"RUN_DIR:\", RUN_DIR)\n",
    "print(\"MODE (env):\", MODE_ENV or \"(not set)\")\n",
    "print(\"YEARS_FILTER:\", YEARS_FILTER, \"MONTHS_FILTER:\", MONTHS_FILTER)\n",
    "\n",
    "# Where to save figures (also show inline)\n",
    "FIG_DIR = REPO_ROOT / \"reports\" / RUN_DIR.name / \"figures\"\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"FIG_DIR:\", FIG_DIR)\n",
    "\n",
    "def savefig(filename: str):\n",
    "    out = FIG_DIR / filename\n",
    "    plt.savefig(out, dpi=200, bbox_inches=\"tight\")\n",
    "    print(\"Saved:\", out)\n",
    "\n",
    "def read_csv(path: Path) -> pd.DataFrame:\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "# ---- Load per-run summary CSVs ----\n",
    "df_year  = read_csv(RUN_DIR / \"citibike_trips_by_year.csv\")\n",
    "df_month = read_csv(RUN_DIR / \"citibike_trips_by_month.csv\")\n",
    "df_dow   = read_csv(RUN_DIR / \"citibike_trips_by_dow.csv\")\n",
    "df_hour  = read_csv(RUN_DIR / \"citibike_trips_by_hour.csv\")\n",
    "\n",
    "# Optional per-run outputs\n",
    "station_path = RUN_DIR / \"citibike_station_exposure.csv\"\n",
    "df_station = read_csv(station_path) if station_path.exists() else None\n",
    "\n",
    "risk_path = RUN_DIR / \"station_risk_exposure_plus_crashproximity.csv\"\n",
    "df_risk = read_csv(risk_path) if risk_path.exists() else None\n",
    "\n",
    "highlights_path = RUN_DIR / \"summary_highlights.md\"\n",
    "\n",
    "# Mode detection (prefer data, else env)\n",
    "mode = (\n",
    "    str(df_year[\"mode\"].iloc[0]).lower()\n",
    "    if (\"mode\" in df_year.columns and len(df_year))\n",
    "    else (MODE_ENV or \"unknown\")\n",
    ")\n",
    "print(\"Detected mode:\", mode)\n",
    "\n",
    "# ---- Compare tables (optional) ----\n",
    "# If YEARS/MONTHS filters are passed (e.g., Janâ€“Mar), we default to THIS run only.\n",
    "USE_COMPARE = (YEARS_FILTER is None and MONTHS_FILTER is None)\n",
    "COMPARE_DIR = SUMMARIES_ROOT / \"_compare\"\n",
    "\n",
    "def _maybe_load_compare(fname: str) -> pd.DataFrame | None:\n",
    "    p = COMPARE_DIR / fname\n",
    "    return read_csv(p) if p.exists() else None\n",
    "\n",
    "if USE_COMPARE and COMPARE_DIR.exists():\n",
    "    _y = _maybe_load_compare(\"citibike_trips_by_year_ALL.csv\")\n",
    "    _m = _maybe_load_compare(\"citibike_trips_by_month_ALL.csv\")\n",
    "    _d = _maybe_load_compare(\"citibike_trips_by_dow_ALL.csv\")\n",
    "    _h = _maybe_load_compare(\"citibike_trips_by_hour_ALL.csv\")\n",
    "    df_year_all  = _y if _y is not None else df_year.copy()\n",
    "    df_month_all = _m if _m is not None else df_month.copy()\n",
    "    df_dow_all   = _d if _d is not None else df_dow.copy()\n",
    "    df_hour_all  = _h if _h is not None else df_hour.copy() \n",
    "else:\n",
    "    df_year_all, df_month_all, df_dow_all, df_hour_all = df_year.copy(), df_month.copy(), df_dow.copy(), df_hour.copy()\n",
    "\n",
    "# Always filter _ALL tables to current mode (if they have a mode column)\n",
    "for _name in [\"df_year_all\", \"df_month_all\", \"df_dow_all\", \"df_hour_all\"]:\n",
    "    _df = locals()[_name]\n",
    "    if isinstance(_df, pd.DataFrame) and (\"mode\" in _df.columns):\n",
    "        locals()[_name] = _df[_df[\"mode\"].astype(str).str.lower() == mode].copy()\n",
    "\n",
    "# If filters are provided, enforce them on the per-run tables too (defensive)\n",
    "def _filter_year_month(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    if YEARS_FILTER is not None and \"year\" in out.columns:\n",
    "        out[\"year\"] = pd.to_numeric(out[\"year\"], errors=\"coerce\")\n",
    "        out = out[out[\"year\"].isin(YEARS_FILTER)]\n",
    "    if MONTHS_FILTER is not None and \"month\" in out.columns:\n",
    "        out[\"month\"] = pd.to_numeric(out[\"month\"], errors=\"coerce\")\n",
    "        out = out[out[\"month\"].isin(MONTHS_FILTER)]\n",
    "    return out\n",
    "\n",
    "df_year  = _filter_year_month(df_year)\n",
    "df_month = _filter_year_month(df_month)\n",
    "df_dow   = _filter_year_month(df_dow)\n",
    "df_hour  = _filter_year_month(df_hour)\n",
    "\n",
    "# Helpful run label for titles\n",
    "run_label = RUN_DIR.name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac9b678",
   "metadata": {},
   "source": [
    "## 8) Risk Deep Dive Analysis\n",
    "\n",
    "Now that we have the AXA scorecard, let's analyze:\n",
    "- **WHERE**: Which stations are highest risk? Do they cluster geographically?\n",
    "- **WHEN**: Does risk vary by hour of day or day of week?\n",
    "- **WHO**: What exposure levels do high-risk stations have?\n",
    "\n",
    "This analysis helps AXA:\n",
    "1. **Price accurately**: Understand risk distribution\n",
    "2. **Target prevention**: Find geographic clusters for safety campaigns\n",
    "3. **Optimize products**: Match offerings to temporal patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04482ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RISK DEEP DIVE ANALYSIS\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'RUN_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Load scorecard (if exists)\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m scorecard_path \u001b[38;5;241m=\u001b[39m \u001b[43mRUN_DIR\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maxa_partner_scorecard_500m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scorecard_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     13\u001b[0m     df_score \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(scorecard_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RUN_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOAD SCORECARD DATA (radius-aware)\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RISK DEEP DIVE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "_RADIUS_RE = re.compile(r\"^\\s*(\\d+(?:\\.\\d+)?)\\s*(m|km)?\\s*$\", re.IGNORECASE)\n",
    "\n",
    "def parse_radius_to_m(raw: str) -> int:\n",
    "    s = (raw or \"\").strip().lower()\n",
    "    if s in (\"\", \"auto\", \"max\"):\n",
    "        return -1\n",
    "    m = _RADIUS_RE.match(s)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Bad AXA_RADIUS={raw!r}. Use like 450m, 750, 1km, auto.\")\n",
    "    val = float(m.group(1))\n",
    "    unit = (m.group(2) or \"m\").lower()\n",
    "    meters = val * (1000.0 if unit == \"km\" else 1.0)\n",
    "    if meters <= 0:\n",
    "        raise ValueError(f\"AXA_RADIUS must be > 0 (got {raw!r})\")\n",
    "    return int(round(meters))\n",
    "\n",
    "def available_scorecards(run_dir: Path) -> list[int]:\n",
    "    radii = []\n",
    "    for p in run_dir.glob(\"axa_partner_scorecard_*m.csv\"):\n",
    "        mm = re.match(r\"^axa_partner_scorecard_(\\d+)m\\.csv$\", p.name)\n",
    "        if mm:\n",
    "            radii.append(int(mm.group(1)))\n",
    "    return sorted(set(radii))\n",
    "\n",
    "def pick_scorecard_path(run_dir: Path, radius_env: str) -> Path | None:\n",
    "    avail = available_scorecards(run_dir)\n",
    "    if not avail:\n",
    "        return None\n",
    "    wanted = parse_radius_to_m(radius_env)\n",
    "    if wanted == -1:\n",
    "        chosen = max(avail)\n",
    "    else:\n",
    "        chosen = wanted if wanted in avail else (500 if 500 in avail else max(avail))\n",
    "    p = run_dir / f\"axa_partner_scorecard_{chosen}m.csv\"\n",
    "    return p if p.exists() else None\n",
    "\n",
    "radius_env = os.environ.get(\"AXA_RADIUS\", \"auto\")\n",
    "scorecard_path = pick_scorecard_path(RUN_DIR, radius_env)\n",
    "\n",
    "if scorecard_path and scorecard_path.exists():\n",
    "    df_score = pd.read_csv(scorecard_path)\n",
    "    print(f\"âœ… Loaded scorecard: {len(df_score):,} stations\")\n",
    "    print(f\"   AXA_RADIUS (env): {radius_env} -> using file: {scorecard_path.name}\")\n",
    "\n",
    "    # Check for credibility column\n",
    "    has_credibility = 'credibility_flag' in df_score.columns\n",
    "    if not has_credibility:\n",
    "        print(\"âš ï¸ Scorecard missing 'credibility_flag' column - analysis will proceed without credibility filtering\")\n",
    "else:\n",
    "    df_score = None\n",
    "    avail = available_scorecards(RUN_DIR)\n",
    "    print(\"âŒ Scorecard not found for this run.\")\n",
    "    print(f\"   AXA_RADIUS (env): {radius_env}\")\n",
    "    print(f\"   Available scorecards in RUN_DIR: {avail}\")\n",
    "if df_score is not None:\n",
    "    print(\"\\nCredibility breakdown:\")\n",
    "    if \"credibility_flag\" in df_score.columns:\n",
    "        print(df_score[\"credibility_flag\"].value_counts())\n",
    "    else:\n",
    "        print(\"(no credibility_flag column)\")\n",
    "\n",
    "    print(\"\\nSample scorecard data:\")\n",
    "    display(df_score.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24896d52",
   "metadata": {},
   "source": [
    "### 8.1 High-Risk Stations: WHO and WHERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e422e3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# --- HIGH-RISK STATIONS ---\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdf_score\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHIGH-RISK STATIONS ANALYSIS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_score' is not defined"
     ]
    }
   ],
   "source": [
    "# --- HIGH-RISK STATIONS ---\n",
    "if df_score is not None:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"HIGH-RISK STATIONS ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Filter to credible high-risk stations (top 10% risk)\n",
    "    credible_stations = df_score[df_score['credibility_flag'] == 'credible'].copy()\n",
    "    \n",
    "    if len(credible_stations) > 0 and 'risk_index_pct' in credible_stations.columns:\n",
    "        high_risk = credible_stations[credible_stations['risk_index_pct'] >= 90].copy()\n",
    "        \n",
    "        print(f\"\\nHigh-risk stations (â‰¥90th percentile): {len(high_risk):,}\")\n",
    "        print(f\"Total exposure at high-risk stations: {high_risk['exposure_trips'].sum():,} trips\")\n",
    "        \n",
    "        # Top 10 highest risk\n",
    "        top_risk = high_risk.nlargest(10, 'eb_risk_rate_per_100k_trips')[[\n",
    "            'start_station_id', 'start_station_name', 'station_lat', 'station_lng',\n",
    "            'exposure_trips', 'crash_count', 'eb_risk_rate_per_100k_trips', 'risk_index_pct'\n",
    "        ]].copy()\n",
    "        \n",
    "        print(\"\\nðŸ“ Top 10 Highest-Risk Stations:\")\n",
    "        display(top_risk)\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # PLOT: Top 10 Highest-Risk Stations\n",
    "        # ------------------------------------------------------------\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            import numpy as np\n",
    "\n",
    "            # Sort for nicer bar order (lowest at bottom, highest at top)\n",
    "            top_plot = top_risk.sort_values('eb_risk_rate_per_100k_trips', ascending=True).copy()\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(11, 6))\n",
    "\n",
    "            y = np.arange(len(top_plot))\n",
    "            ax.barh(y, top_plot['eb_risk_rate_per_100k_trips'], edgecolor='black')\n",
    "\n",
    "            ax.set_yticks(y)\n",
    "            ax.set_yticklabels([str(n)[:40] for n in top_plot['start_station_name']], fontsize=9)\n",
    "            ax.set_xlabel('E-bike Risk Rate per 100k Trips', fontsize=11)\n",
    "            ax.set_title('Top 10 Highest-Risk Stations (Credible, â‰¥90th Percentile)', fontsize=12, fontweight='bold')\n",
    "            ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            savefig(\"top10_highest_risk_stations.png\")\n",
    "            plt.show()\n",
    "\n",
    "            print(\"âœ“ Saved plot: top10_highest_risk_stations.png\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Could not generate plot: {e}\")\n",
    "\n",
    "        # Geographic clustering analysis\n",
    "        if len(high_risk) > 1:\n",
    "            try:\n",
    "                from sklearn.cluster import DBSCAN\n",
    "                import numpy as np\n",
    "                \n",
    "                # Cluster high-risk stations geographically\n",
    "                coords = high_risk[['station_lat', 'station_lng']].values\n",
    "                \n",
    "                # DBSCAN clustering (0.01 degrees â‰ˆ 1km)\n",
    "                clustering = DBSCAN(eps=0.01, min_samples=2).fit(coords)\n",
    "                high_risk['cluster'] = clustering.labels_\n",
    "                \n",
    "                # Count clusters\n",
    "                n_clusters = len(set(clustering.labels_)) - (1 if -1 in clustering.labels_ else 0)\n",
    "                n_noise = list(clustering.labels_).count(-1)\n",
    "                \n",
    "                print(f\"\\nðŸ—ºï¸  Geographic Clustering:\")\n",
    "                print(f\"  High-risk clusters identified: {n_clusters}\")\n",
    "                print(f\"  Isolated high-risk stations: {n_noise}\")\n",
    "                \n",
    "                # Show cluster details\n",
    "                if n_clusters > 0:\n",
    "                    cluster_summary = high_risk[high_risk['cluster'] >= 0].groupby('cluster').agg({\n",
    "                        'start_station_id': 'count',\n",
    "                        'exposure_trips': 'sum',\n",
    "                        'crash_count': 'sum',\n",
    "                        'station_lat': 'mean',\n",
    "                        'station_lng': 'mean'\n",
    "                    }).rename(columns={'start_station_id': 'stations_in_cluster'})\n",
    "                    \n",
    "                    print(\"\\nCluster Details:\")\n",
    "                    display(cluster_summary)\n",
    "            except ImportError:\n",
    "                print(\"âš ï¸  sklearn not installed - skipping geographic clustering\")\n",
    "                print(\"   Install with: pip install scikit-learn\")\n",
    "    else:\n",
    "        print(\"âš ï¸  No credible stations with risk data available\")\n",
    "else:\n",
    "    print(\"âš ï¸  df_score is None (no scoring data available)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0327da89",
   "metadata": {},
   "source": [
    "### 8.2 Risk Distribution Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90639ef1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# --- RISK DISTRIBUTION ---\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdf_score\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(credible_stations) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRISK DISTRIBUTION ANALYSIS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_score' is not defined"
     ]
    }
   ],
   "source": [
    "# --- RISK DISTRIBUTION ---\n",
    "if df_score is not None and len(credible_stations) > 0:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"RISK DISTRIBUTION ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 1. Risk Histogram\n",
    "    ax = axes[0, 0]\n",
    "    ax.hist(credible_stations['eb_risk_rate_per_100k_trips'].dropna(), \n",
    "            bins=30, edgecolor='black', alpha=0.7, color='#2E86AB')\n",
    "    ax.axvline(credible_stations['eb_risk_rate_per_100k_trips'].median(), \n",
    "               color='red', linestyle='--', label='Median', linewidth=2)\n",
    "    ax.set_xlabel('EB Risk Rate (per 100k trips)', fontsize=11)\n",
    "    ax.set_ylabel('Number of Stations', fontsize=11)\n",
    "    ax.set_title('Risk Distribution (Credible Stations Only)', fontsize=12, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    # 2. Exposure vs Risk Scatter\n",
    "    ax = axes[0, 1]\n",
    "    scatter = ax.scatter(\n",
    "        credible_stations['exposure_trips'], \n",
    "        credible_stations['eb_risk_rate_per_100k_trips'],\n",
    "        c=credible_stations['crash_count'],\n",
    "        cmap='YlOrRd',\n",
    "        alpha=0.6,\n",
    "        s=50\n",
    "    )\n",
    "    ax.set_xlabel('Exposure (trips)', fontsize=11)\n",
    "    ax.set_ylabel('EB Risk Rate (per 100k trips)', fontsize=11)\n",
    "    ax.set_title('Risk vs Exposure', fontsize=12, fontweight='bold')\n",
    "    ax.set_xscale('log')\n",
    "    plt.colorbar(scatter, ax=ax, label='Crash Count')\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    # Mark prevention hotspots\n",
    "    if 'prevention_hotspot' in credible_stations.columns:\n",
    "        hotspots = credible_stations[credible_stations['prevention_hotspot'] == True]\n",
    "        if len(hotspots) > 0:\n",
    "            ax.scatter(hotspots['exposure_trips'], \n",
    "                      hotspots['eb_risk_rate_per_100k_trips'],\n",
    "                      color='red', s=200, marker='*', \n",
    "                      edgecolors='black', linewidths=1,\n",
    "                      label='Prevention Hotspots', zorder=5)\n",
    "            ax.legend()\n",
    "    \n",
    "    # 3. Risk Tiers\n",
    "    ax = axes[1, 0]\n",
    "    if 'risk_index_pct' in credible_stations.columns:\n",
    "        # Create risk tiers\n",
    "        credible_stations['risk_tier'] = pd.cut(\n",
    "            credible_stations['risk_index_pct'],\n",
    "            bins=[0, 33, 66, 100],\n",
    "            labels=['Low Risk', 'Medium Risk', 'High Risk']\n",
    "        )\n",
    "        \n",
    "        tier_counts = credible_stations['risk_tier'].value_counts()\n",
    "        colors = ['#4CAF50', '#FFC107', '#F44336']\n",
    "        tier_counts.plot(kind='bar', ax=ax, color=colors, edgecolor='black')\n",
    "        ax.set_xlabel('Risk Tier', fontsize=11)\n",
    "        ax.set_ylabel('Number of Stations', fontsize=11)\n",
    "        ax.set_title('Station Distribution by Risk Tier', fontsize=12, fontweight='bold')\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
    "        ax.grid(alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add percentages on bars\n",
    "        for i, v in enumerate(tier_counts):\n",
    "            pct = v / len(credible_stations) * 100\n",
    "            ax.text(i, v, f'{v}\\n({pct:.1f}%)', \n",
    "                   ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 4. Expected Incidents Distribution\n",
    "    ax = axes[1, 1]\n",
    "    if 'expected_incidents_proxy' in credible_stations.columns:\n",
    "        top_20 = credible_stations.nlargest(20, 'expected_incidents_proxy')\n",
    "        ax.barh(range(len(top_20)), top_20['expected_incidents_proxy'], \n",
    "                color='#E74C3C', edgecolor='black')\n",
    "        ax.set_yticks(range(len(top_20)))\n",
    "        ax.set_yticklabels([f\"{row['start_station_name'][:30]}...\" \n",
    "                           if len(row['start_station_name']) > 30 \n",
    "                           else row['start_station_name']\n",
    "                           for _, row in top_20.iterrows()], fontsize=8)\n",
    "        ax.set_xlabel('Expected Incidents Proxy', fontsize=11)\n",
    "        ax.set_title('Top 20 Stations by Expected Impact', fontsize=12, fontweight='bold')\n",
    "        ax.grid(alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    savefig(\"08_risk_distribution_analysis.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f431626",
   "metadata": {},
   "source": [
    "### 8.3 Temporal Risk Patterns: WHEN does risk peak?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31b78e88",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_hour' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# --- TEMPORAL PATTERNS: HOUR OF DAY ---\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdf_hour\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTEMPORAL PATTERNS: HOUR OF DAY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_hour' is not defined"
     ]
    }
   ],
   "source": [
    "# --- TEMPORAL PATTERNS: HOUR OF DAY ---\n",
    "if df_hour is not None:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEMPORAL PATTERNS: HOUR OF DAY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Hour data is already aggregated system-wide\n",
    "    hour_summary = df_hour.groupby('hour')['trips'].sum().reset_index()\n",
    "    hour_summary = hour_summary.sort_values('hour')\n",
    "    \n",
    "    # Calculate percentage of daily trips\n",
    "    total_trips = hour_summary['trips'].sum()\n",
    "    hour_summary['pct_of_daily'] = (hour_summary['trips'] / total_trips) * 100\n",
    "    \n",
    "    # Plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # 1. Trip volume by hour\n",
    "    ax1.bar(hour_summary['hour'], hour_summary['trips'], \n",
    "           color='#2E86AB', edgecolor='black', alpha=0.7)\n",
    "    ax1.set_xlabel('Hour of Day', fontsize=12)\n",
    "    ax1.set_ylabel('Total Trips', fontsize=12)\n",
    "    ax1.set_title(f'Trip Volume by Hour â€” mode={mode}', fontsize=13, fontweight='bold')\n",
    "    ax1.set_xticks(range(0, 24))\n",
    "    ax1.grid(alpha=0.3, axis='y')\n",
    "    \n",
    "    # Mark rush hours\n",
    "    rush_hours = [8, 9, 17, 18]\n",
    "    for rh in rush_hours:\n",
    "        ax1.axvline(rh, color='orange', linestyle='--', alpha=0.6, linewidth=2)\n",
    "    \n",
    "    # 2. Percentage distribution\n",
    "    ax2.plot(hour_summary['hour'], hour_summary['pct_of_daily'], \n",
    "            marker='o', linewidth=2, markersize=8, color='#E74C3C')\n",
    "    ax2.fill_between(hour_summary['hour'], hour_summary['pct_of_daily'], \n",
    "                     alpha=0.3, color='#E74C3C')\n",
    "    ax2.set_xlabel('Hour of Day', fontsize=12)\n",
    "    ax2.set_ylabel('% of Daily Trips', fontsize=12)\n",
    "    ax2.set_title('Distribution of Trips Throughout Day', fontsize=13, fontweight='bold')\n",
    "    ax2.set_xticks(range(0, 24))\n",
    "    ax2.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    savefig(\"09_trips_by_hour.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nðŸ“Š Hour-of-Day Analysis:\")\n",
    "    top_hours = hour_summary.nlargest(5, 'trips')[['hour', 'trips', 'pct_of_daily']]\n",
    "    print(\"\\nTop 5 Busiest Hours:\")\n",
    "    display(top_hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1956d61",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_dow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# --- TEMPORAL PATTERNS: DAY OF WEEK ---\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdf_dow\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTEMPORAL PATTERNS: DAY OF WEEK\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_dow' is not defined"
     ]
    }
   ],
   "source": [
    "# --- TEMPORAL PATTERNS: DAY OF WEEK ---\n",
    "if df_dow is not None:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEMPORAL PATTERNS: DAY OF WEEK\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Day-of-week data is already aggregated system-wide\n",
    "    dow_summary = df_dow.groupby(['dow', 'dow_name']).agg({\n",
    "        'trips': 'sum'\n",
    "    }).reset_index()\n",
    "    dow_summary = dow_summary.sort_values('dow')\n",
    "    \n",
    "    # Calculate percentage of weekly trips\n",
    "    total_trips = dow_summary['trips'].sum()\n",
    "    dow_summary['pct_of_weekly'] = (dow_summary['trips'] / total_trips) * 100\n",
    "    \n",
    "    # Plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # 1. Trip volume by day of week\n",
    "    colors = ['#4CAF50' if 'Saturday' in str(x) or 'Sunday' in str(x) else '#2E86AB' \n",
    "              for x in dow_summary['dow_name']]\n",
    "    \n",
    "    ax1.bar(dow_summary['dow_name'], dow_summary['trips'], \n",
    "           color=colors, edgecolor='black', alpha=0.7)\n",
    "    ax1.set_xlabel('Day of Week', fontsize=11)\n",
    "    ax1.set_ylabel('Total Trips', fontsize=11)\n",
    "    ax1.set_title(f'Trip Volume by Day of Week â€” mode={mode}', fontsize=12, fontweight='bold')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.grid(alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (idx, row) in enumerate(dow_summary.iterrows()):\n",
    "        ax1.text(i, row['trips'], f\"{row['trips']:,.0f}\",\n",
    "                ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    # 2. Weekday vs Weekend comparison\n",
    "    if 'week_part' in df_dow.columns:\n",
    "        week_part_summary = df_dow.groupby('week_part').agg({\n",
    "            'trips': 'sum'\n",
    "        }).reset_index()\n",
    "        \n",
    "        week_part_summary['pct'] = (week_part_summary['trips'] / week_part_summary['trips'].sum()) * 100\n",
    "        \n",
    "        colors_wp = ['#2E86AB' if 'weekday' in str(x) else '#4CAF50' \n",
    "                     for x in week_part_summary['week_part']]\n",
    "        \n",
    "        ax2.bar(week_part_summary['week_part'], week_part_summary['trips'],\n",
    "               color=colors_wp, edgecolor='black', width=0.6, alpha=0.7)\n",
    "        ax2.set_xlabel('Week Part', fontsize=11)\n",
    "        ax2.set_ylabel('Total Trips', fontsize=11)\n",
    "        ax2.set_title('Weekday vs Weekend', fontsize=12, fontweight='bold')\n",
    "        ax2.grid(alpha=0.3, axis='y')\n",
    "        \n",
    "        for i, (idx, row) in enumerate(week_part_summary.iterrows()):\n",
    "            ax2.text(i, row['trips'], \n",
    "                    f\"{row['trips']:,.0f}\\n({row['pct']:.1f}%)\",\n",
    "                    ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    savefig(\"10_trips_by_day_of_week.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nðŸ“Š Day-of-Week Analysis:\")\n",
    "    display(dow_summary[['dow_name', 'trips', 'pct_of_weekly']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8abc4e",
   "metadata": {},
   "source": [
    "### 8.4 Executive Risk Summary for AXA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "338d4bbb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# --- EXECUTIVE RISK SUMMARY ---\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdf_score\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEXECUTIVE RISK SUMMARY FOR AXA\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_score' is not defined"
     ]
    }
   ],
   "source": [
    "# --- EXECUTIVE RISK SUMMARY ---\n",
    "if df_score is not None:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXECUTIVE RISK SUMMARY FOR AXA\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Create summary metrics\n",
    "    summary_data = []\n",
    "    \n",
    "    # Overall metrics\n",
    "    total_stations = len(df_score)\n",
    "    credible_count = len(df_score[df_score['credibility_flag'] == 'credible'])\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Metric': 'Total Stations',\n",
    "        'Value': f\"{total_stations:,}\",\n",
    "        'Notes': f\"{mode.upper()} mode\"\n",
    "    })\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Metric': 'Credible Stations (â‰¥5K trips)',\n",
    "        'Value': f\"{credible_count:,} ({credible_count/total_stations*100:.1f}%)\",\n",
    "        'Notes': 'Suitable for risk-based pricing'\n",
    "    })\n",
    "    \n",
    "    # Risk tiers\n",
    "    if 'risk_tier' in credible_stations.columns:\n",
    "        high_risk_count = len(credible_stations[credible_stations['risk_tier'] == 'High Risk'])\n",
    "        medium_risk_count = len(credible_stations[credible_stations['risk_tier'] == 'Medium Risk'])\n",
    "        low_risk_count = len(credible_stations[credible_stations['risk_tier'] == 'Low Risk'])\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Metric': 'High Risk Stations',\n",
    "            'Value': f\"{high_risk_count:,} ({high_risk_count/credible_count*100:.1f}%)\",\n",
    "            'Notes': 'Premium pricing tier'\n",
    "        })\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Metric': 'Medium Risk Stations',\n",
    "            'Value': f\"{medium_risk_count:,} ({medium_risk_count/credible_count*100:.1f}%)\",\n",
    "            'Notes': 'Standard pricing tier'\n",
    "        })\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Metric': 'Low Risk Stations',\n",
    "            'Value': f\"{low_risk_count:,} ({low_risk_count/credible_count*100:.1f}%)\",\n",
    "            'Notes': 'Discount pricing tier'\n",
    "        })\n",
    "    \n",
    "    # Hotspots\n",
    "    if 'prevention_hotspot' in df_score.columns:\n",
    "        prevention_count = df_score['prevention_hotspot'].sum()\n",
    "        summary_data.append({\n",
    "            'Metric': 'Prevention Hotspots',\n",
    "            'Value': f\"{int(prevention_count):,}\",\n",
    "            'Notes': 'High exposure + High risk â†’ Safety campaigns'\n",
    "        })\n",
    "    \n",
    "    if 'product_hotspot' in df_score.columns:\n",
    "        product_count = df_score['product_hotspot'].sum()\n",
    "        summary_data.append({\n",
    "            'Metric': 'Product Hotspots',\n",
    "            'Value': f\"{int(product_count):,}\",\n",
    "            'Notes': 'High exposure â†’ Insurance sales opportunity'\n",
    "        })\n",
    "    \n",
    "    if 'acquisition_hotspot' in df_score.columns:\n",
    "        acquisition_count = df_score['acquisition_hotspot'].sum()\n",
    "        summary_data.append({\n",
    "            'Metric': 'Acquisition Hotspots',\n",
    "            'Value': f\"{int(acquisition_count):,}\",\n",
    "            'Notes': 'High exposure + Low risk â†’ Easy wins'\n",
    "        })\n",
    "    \n",
    "    # Display summary table\n",
    "    df_summary = pd.DataFrame(summary_data)\n",
    "    display(df_summary)\n",
    "    \n",
    "    # Save to CSV\n",
    "    summary_csv = RUN_DIR / \"risk_executive_summary.csv\"\n",
    "    df_summary.to_csv(summary_csv, index=False)\n",
    "    print(f\"\\nâœ… Saved executive summary to: {summary_csv}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "modified_at": "2026-01-03T11:12:10.237164Z",
  "modified_by": "chatgpt"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
