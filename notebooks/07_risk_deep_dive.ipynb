{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8edcd816",
   "metadata": {},
   "source": [
    "# Risk Deep Dive: Geographic & Temporal Analysis\n",
    "   \n",
    "   Detailed risk analysis for AXA Deutschland bike-share insurance pricing.\n",
    "   \n",
    "   **Prerequisites**: Run `make scorecard MODE=nyc` first.\n",
    "   \n",
    "   **Audience**: Technical teams, actuaries, data scientists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1edc51",
   "metadata": {},
   "source": [
    "## How to run (recommended)\n",
    "\n",
    "From the repo root:\n",
    "\n",
    "```bash\n",
    "make all MODE=nyc YEARS=\"2023 2024\" MONTHS=\"1 2 3\"\n",
    "make report MODE=nyc YEARS=\"2023 2024\" MONTHS=\"1 2 3\"\n",
    "```\n",
    "\n",
    "The Makefile sets environment variables (e.g. `CITIBIKE_PARQUET_DIR`, `CITIBIKE_YEARS`, `CITIBIKE_MONTHS`) which this notebook reads.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b53285",
   "metadata": {},
   "source": [
    "## Analysis Overview\n",
    "   \n",
    "   This notebook provides detailed risk analysis including:\n",
    "   1. **High-Risk Station Identification** - Top 10% riskiest stations\n",
    "   2. **Geographic Clustering** - Do high-risk stations cluster?\n",
    "   3. **Risk Distribution** - How is risk distributed across the system?\n",
    "   4. **Temporal Patterns** - When is exposure highest?\n",
    "   5. **Executive Summary** - AXA-ready metrics\n",
    "   \n",
    "   **Run time**: ~2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9f74bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display, Markdown\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# --- Setup (STRICT): load summaries + FORCE year/month risk tables (NO overall fallback) ---\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Notebook-friendly display\n",
    "try:\n",
    "    from IPython.display import display, Markdown\n",
    "except Exception:\n",
    "    display = print\n",
    "    Markdown = lambda x: x\n",
    "\n",
    "# Ensure figures render in executed notebook\n",
    "try:\n",
    "    get_ipython().run_line_magic(\"matplotlib\", \"inline\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "plt.ioff()  # nbconvert-friendly\n",
    "\n",
    "# ---------------------------\n",
    "# Repo discovery\n",
    "# ---------------------------\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    start = start.resolve()\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"Makefile\").exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(f\"Could not find repo root (Makefile) from CWD={Path.cwd().resolve()}\")\n",
    "\n",
    "REPO_ROOT = find_repo_root(Path.cwd())\n",
    "SUMMARIES_ROOT = REPO_ROOT / \"summaries\"\n",
    "\n",
    "# ---------------------------\n",
    "# Helpers\n",
    "# ---------------------------\n",
    "def _parse_int_list(val: str | None):\n",
    "    if val is None:\n",
    "        return None\n",
    "    s = str(val).strip()\n",
    "    if not s:\n",
    "        return None\n",
    "    parts = re.split(r\"[,\\s]+\", s)\n",
    "    out = []\n",
    "    for p in parts:\n",
    "        if not p:\n",
    "            continue\n",
    "        try:\n",
    "            out.append(int(p))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return out or None\n",
    "\n",
    "def read_csv_strict(path: Path) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing required CSV: {path}\")\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def read_csv_optional(path: Path) -> pd.DataFrame | None:\n",
    "    return pd.read_csv(path) if path.exists() else None\n",
    "\n",
    "def _filter_year_month(df: pd.DataFrame, years: list[int] | None, months: list[int] | None) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    if years is not None and \"year\" in out.columns:\n",
    "        out[\"year\"] = pd.to_numeric(out[\"year\"], errors=\"coerce\")\n",
    "        out = out[out[\"year\"].isin(years)]\n",
    "    if months is not None and \"month\" in out.columns:\n",
    "        out[\"month\"] = pd.to_numeric(out[\"month\"], errors=\"coerce\")\n",
    "        out = out[out[\"month\"].isin(months)]\n",
    "    return out\n",
    "\n",
    "# ---------------------------\n",
    "# Inputs from Makefile\n",
    "# ---------------------------\n",
    "PARQUET_DIR_ENV = (os.environ.get(\"CITIBIKE_PARQUET_DIR\") or \"\").strip()\n",
    "RUN_DIR_ENV     = (os.environ.get(\"CITIBIKE_RUN_DIR\") or \"\").strip()\n",
    "MODE_ENV        = (os.environ.get(\"CITIBIKE_MODE\") or os.environ.get(\"MODE\") or \"\").strip().lower()\n",
    "\n",
    "YEARS_FILTER  = _parse_int_list(os.environ.get(\"CITIBIKE_YEARS\")  or os.environ.get(\"YEARS\"))\n",
    "MONTHS_FILTER = _parse_int_list(os.environ.get(\"CITIBIKE_MONTHS\") or os.environ.get(\"MONTHS\"))\n",
    "\n",
    "PARQUET_DIR = Path(PARQUET_DIR_ENV) if PARQUET_DIR_ENV else Path()\n",
    "\n",
    "if RUN_DIR_ENV:\n",
    "    RUN_DIR = Path(RUN_DIR_ENV)\n",
    "else:\n",
    "    run_tag = PARQUET_DIR.name if str(PARQUET_DIR).strip() else \"\"\n",
    "    RUN_DIR = (SUMMARIES_ROOT / run_tag) if run_tag else Path()\n",
    "\n",
    "# Resolve relative -> absolute\n",
    "if str(RUN_DIR).strip() and not RUN_DIR.is_absolute():\n",
    "    RUN_DIR = (REPO_ROOT / RUN_DIR).resolve()\n",
    "if str(PARQUET_DIR).strip() and not PARQUET_DIR.is_absolute():\n",
    "    PARQUET_DIR = (REPO_ROOT / PARQUET_DIR).resolve()\n",
    "\n",
    "# ---------------------------\n",
    "# Strict checks\n",
    "# ---------------------------\n",
    "if not SUMMARIES_ROOT.exists():\n",
    "    raise FileNotFoundError(f\"Expected summaries/ folder at: {SUMMARIES_ROOT}\")\n",
    "if not RUN_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Expected run summaries at: {RUN_DIR}\")\n",
    "\n",
    "print(\"REPO_ROOT:\", REPO_ROOT)\n",
    "print(\"RUN_DIR:\", RUN_DIR)\n",
    "print(\"PARQUET_DIR:\", PARQUET_DIR if str(PARQUET_DIR).strip() else \"(not set)\")\n",
    "print(\"MODE (env):\", MODE_ENV or \"(not set)\")\n",
    "print(\"YEARS_FILTER:\", YEARS_FILTER, \"MONTHS_FILTER:\", MONTHS_FILTER)\n",
    "\n",
    "# Figures dir\n",
    "FIG_DIR = REPO_ROOT / \"reports\" / RUN_DIR.name / \"figures\"\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"FIG_DIR:\", FIG_DIR)\n",
    "\n",
    "def savefig(filename: str):\n",
    "    out = FIG_DIR / filename\n",
    "    plt.savefig(out, dpi=200, bbox_inches=\"tight\")\n",
    "    print(\"Saved:\", out)\n",
    "\n",
    "# ---------------------------\n",
    "# Load core per-run summaries (required)\n",
    "# ---------------------------\n",
    "df_year  = read_csv_strict(RUN_DIR / \"citibike_trips_by_year.csv\")\n",
    "df_month = read_csv_strict(RUN_DIR / \"citibike_trips_by_month.csv\")\n",
    "df_dow   = read_csv_strict(RUN_DIR / \"citibike_trips_by_dow.csv\")\n",
    "df_hour  = read_csv_strict(RUN_DIR / \"citibike_trips_by_hour.csv\")\n",
    "\n",
    "# Optional outputs\n",
    "df_station = read_csv_optional(RUN_DIR / \"citibike_station_exposure.csv\")\n",
    "\n",
    "# ---------------------------\n",
    "# Load RISK (FORCE granular)\n",
    "# ---------------------------\n",
    "risk_year_path = RUN_DIR / \"station_risk_exposure_plus_crashproximity_by_year.csv\"\n",
    "risk_ym_path   = RUN_DIR / \"station_risk_exposure_plus_crashproximity_by_year_month.csv\"\n",
    "\n",
    "df_risk_year = read_csv_optional(risk_year_path)\n",
    "df_risk_ym   = read_csv_optional(risk_ym_path)\n",
    "\n",
    "# Force granular selection (no overall fallback)\n",
    "if df_risk_ym is not None:\n",
    "    df_risk = df_risk_ym\n",
    "    risk_source = \"by_year_month\"\n",
    "elif df_risk_year is not None:\n",
    "    df_risk = df_risk_year\n",
    "    risk_source = \"by_year\"\n",
    "else:\n",
    "    df_risk = None\n",
    "    risk_source = \"missing\"\n",
    "    raise FileNotFoundError(\n",
    "        \"No per-year/per-month risk CSVs found in RUN_DIR.\\n\"\n",
    "        f\"Expected one of:\\n  - {risk_ym_path}\\n  - {risk_year_path}\\n\"\n",
    "        \"If you truly want overall-only risk, load station_risk_exposure_plus_crashproximity.csv explicitly (but you said you don't).\"\n",
    "    )\n",
    "\n",
    "# Highlights\n",
    "highlights_path = RUN_DIR / \"summary_highlights.md\"\n",
    "\n",
    "# Mode detection\n",
    "mode = (\n",
    "    str(df_year[\"mode\"].iloc[0]).lower()\n",
    "    if (\"mode\" in df_year.columns and len(df_year))\n",
    "    else (MODE_ENV or \"unknown\")\n",
    ")\n",
    "print(\"Detected mode:\", mode)\n",
    "\n",
    "# Apply filters defensively\n",
    "df_year  = _filter_year_month(df_year,  YEARS_FILTER, MONTHS_FILTER)\n",
    "df_month = _filter_year_month(df_month, YEARS_FILTER, MONTHS_FILTER)\n",
    "df_dow   = _filter_year_month(df_dow,   YEARS_FILTER, MONTHS_FILTER)\n",
    "df_hour  = _filter_year_month(df_hour,  YEARS_FILTER, MONTHS_FILTER)\n",
    "\n",
    "df_risk  = _filter_year_month(df_risk,  YEARS_FILTER, MONTHS_FILTER)\n",
    "\n",
    "run_label = RUN_DIR.name\n",
    "\n",
    "print(\"\\nRisk files found:\")\n",
    "print(\" - by_year:\", \"YES\" if df_risk_year is not None else \"NO\")\n",
    "print(\" - by_year_month:\", \"YES\" if df_risk_ym is not None else \"NO\")\n",
    "print(\"Using df_risk =\", risk_source)\n",
    "print(\"df_risk columns:\", list(df_risk.columns))\n",
    "print(\"Unique years in df_risk:\", sorted(pd.to_numeric(df_risk[\"year\"], errors=\"coerce\").dropna().unique().astype(int).tolist()) if \"year\" in df_risk.columns else \"(no year)\")\n",
    "print(\"Unique months in df_risk:\", sorted(pd.to_numeric(df_risk[\"month\"], errors=\"coerce\").dropna().unique().astype(int).tolist()) if \"month\" in df_risk.columns else \"(no month)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac9b678",
   "metadata": {},
   "source": [
    "## 8) Risk Deep Dive Analysis\n",
    "\n",
    "Now that we have the AXA scorecard, let's analyze:\n",
    "- **WHERE**: Which stations are highest risk? Do they cluster geographically?\n",
    "- **WHEN**: Does risk vary by hour of day or day of week?\n",
    "- **WHO**: What exposure levels do high-risk stations have?\n",
    "\n",
    "This analysis helps AXA:\n",
    "1. **Price accurately**: Understand risk distribution\n",
    "2. **Target prevention**: Find geographic clusters for safety campaigns\n",
    "3. **Optimize products**: Match offerings to temporal patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04482ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RISK DEEP DIVE ANALYSIS\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'RUN_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Load scorecard (if exists)\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m scorecard_path \u001b[38;5;241m=\u001b[39m \u001b[43mRUN_DIR\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maxa_partner_scorecard_500m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scorecard_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     13\u001b[0m     df_score \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(scorecard_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RUN_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOAD SCORECARD (overall) + SHOW RISK separated by YEAR and YEAR-MONTH\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RISK DEEP DIVE (Scorecard overall + Risk separated by time)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ---------------------------\n",
    "# Scorecard loading (overall, by design)\n",
    "# ---------------------------\n",
    "_RADIUS_RE = re.compile(r\"^\\s*(\\d+(?:\\.\\d+)?)\\s*(m|km)?\\s*$\", re.IGNORECASE)\n",
    "\n",
    "def parse_radius_to_m(raw: str) -> int:\n",
    "    s = (raw or \"\").strip().lower()\n",
    "    if s in (\"\", \"auto\", \"max\"):\n",
    "        return -1\n",
    "    m = _RADIUS_RE.match(s)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Bad AXA_RADIUS={raw!r}. Use like 450m, 750, 1km, auto.\")\n",
    "    val = float(m.group(1))\n",
    "    unit = (m.group(2) or \"m\").lower()\n",
    "    meters = val * (1000.0 if unit == \"km\" else 1.0)\n",
    "    if meters <= 0:\n",
    "        raise ValueError(f\"AXA_RADIUS must be > 0 (got {raw!r})\")\n",
    "    return int(round(meters))\n",
    "\n",
    "def available_scorecards(run_dir: Path) -> list[int]:\n",
    "    radii = []\n",
    "    for p in run_dir.glob(\"axa_partner_scorecard_*m.csv\"):\n",
    "        mm = re.match(r\"^axa_partner_scorecard_(\\d+)m\\.csv$\", p.name)\n",
    "        if mm:\n",
    "            radii.append(int(mm.group(1)))\n",
    "    return sorted(set(radii))\n",
    "\n",
    "def pick_scorecard_path(run_dir: Path, radius_env: str) -> Path | None:\n",
    "    avail = available_scorecards(run_dir)\n",
    "    if not avail:\n",
    "        return None\n",
    "    wanted = parse_radius_to_m(radius_env)\n",
    "    chosen = max(avail) if wanted == -1 else (wanted if wanted in avail else (500 if 500 in avail else max(avail)))\n",
    "    p = run_dir / f\"axa_partner_scorecard_{chosen}m.csv\"\n",
    "    return p if p.exists() else None\n",
    "\n",
    "radius_env = os.environ.get(\"AXA_RADIUS\", \"auto\")\n",
    "scorecard_path = pick_scorecard_path(RUN_DIR, radius_env)\n",
    "\n",
    "if scorecard_path and scorecard_path.exists():\n",
    "    df_score = pd.read_csv(scorecard_path)\n",
    "    print(f\"‚úÖ Loaded scorecard (overall): {len(df_score):,} stations\")\n",
    "    print(f\"   AXA_RADIUS (env): {radius_env} -> using file: {scorecard_path.name}\")\n",
    "    display(df_score.head(5))\n",
    "else:\n",
    "    df_score = None\n",
    "    print(\"‚ùå Scorecard not found. Available:\", available_scorecards(RUN_DIR))\n",
    "\n",
    "# ---------------------------\n",
    "# Risk sanity (guaranteed granular by setup cell)\n",
    "# ---------------------------\n",
    "if df_risk is None:\n",
    "    raise RuntimeError(\"df_risk is None (setup should have forced granular risk).\")\n",
    "\n",
    "if \"year\" not in df_risk.columns:\n",
    "    raise RuntimeError(f\"df_risk unexpectedly has no 'year' column. Columns: {list(df_risk.columns)}\")\n",
    "\n",
    "r = df_risk.copy()\n",
    "r[\"year\"] = pd.to_numeric(r[\"year\"], errors=\"coerce\")\n",
    "if \"month\" in r.columns:\n",
    "    r[\"month\"] = pd.to_numeric(r[\"month\"], errors=\"coerce\")\n",
    "\n",
    "# Find crash radius columns (crashes_within_<R>m)\n",
    "crash_cols = [c for c in r.columns if re.match(r\"^crashes_within_\\d+m$\", str(c))]\n",
    "if not crash_cols:\n",
    "    raise RuntimeError(f\"No crashes_within_<R>m columns in df_risk. Columns: {list(r.columns)}\")\n",
    "\n",
    "# Pick one radius for ‚Äúsummary‚Äù tables (prefer 500m)\n",
    "crash_col = \"crashes_within_500m\" if \"crashes_within_500m\" in crash_cols else crash_cols[0]\n",
    "print(\"\\nUsing crash_col for summaries:\", crash_col)\n",
    "\n",
    "# Clean trips\n",
    "r[\"trips\"] = pd.to_numeric(r.get(\"trips\", pd.NA), errors=\"coerce\")\n",
    "r = r.dropna(subset=[\"year\", \"trips\"]).copy()\n",
    "r = r[r[\"trips\"] > 0].copy()\n",
    "\n",
    "# ---------------------------\n",
    "# YEARLY summary (separated)\n",
    "# ---------------------------\n",
    "yearly = (\n",
    "    r.groupby([\"year\"], as_index=False)\n",
    "     .agg(trips=(\"trips\", \"sum\"), crashes=(crash_col, \"sum\"))\n",
    "     .sort_values(\"year\")\n",
    ")\n",
    "yearly[\"crashes_per_100k_trips\"] = yearly[\"crashes\"] / yearly[\"trips\"] * 100000.0\n",
    "\n",
    "print(\"\\n=== Yearly crash-proximity summary ===\")\n",
    "display(yearly)\n",
    "\n",
    "# ---------------------------\n",
    "# YEAR-MONTH summary (separated, if month exists)\n",
    "# ---------------------------\n",
    "if \"month\" in r.columns:\n",
    "    ym = (\n",
    "        r.dropna(subset=[\"month\"])\n",
    "         .groupby([\"year\",\"month\"], as_index=False)\n",
    "         .agg(trips=(\"trips\",\"sum\"), crashes=(crash_col,\"sum\"))\n",
    "         .sort_values([\"year\",\"month\"])\n",
    "    )\n",
    "    ym[\"crashes_per_100k_trips\"] = ym[\"crashes\"] / ym[\"trips\"] * 100000.0\n",
    "\n",
    "    print(\"\\n=== Year-Month crash-proximity summary (first rows) ===\")\n",
    "    display(ym.head(24))\n",
    "\n",
    "    # show each year separately (so it never ‚Äúlooks overall‚Äù)\n",
    "    for yy in sorted(ym[\"year\"].dropna().unique().astype(int).tolist()):\n",
    "        print(f\"\\n--- Year {yy} month breakdown ---\")\n",
    "        display(ym[ym[\"year\"] == yy].copy())\n",
    "else:\n",
    "    print(\"\\n(df_risk has no 'month' column; only yearly separation is possible.)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24896d52",
   "metadata": {},
   "source": [
    "### 8.1 High-Risk Stations: WHO and WHERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e422e3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# --- HIGH-RISK STATIONS ---\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdf_score\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHIGH-RISK STATIONS ANALYSIS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_score' is not defined"
     ]
    }
   ],
   "source": [
    "# --- HIGH-RISK STATIONS (robust + consistent columns) ---\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if df_score is None or len(df_score) == 0:\n",
    "    print(\"‚ö†Ô∏è  df_score is None/empty (no scoring data available)\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"HIGH-RISK STATIONS ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    s = df_score.copy()\n",
    "\n",
    "    # ---- Resolve column names robustly ----\n",
    "    id_col   = \"start_station_id\"   if \"start_station_id\"   in s.columns else (\"station_id\"   if \"station_id\"   in s.columns else None)\n",
    "    name_col = \"start_station_name\" if \"start_station_name\" in s.columns else (\"station_name\" if \"station_name\" in s.columns else None)\n",
    "    lat_col  = \"station_lat\" if \"station_lat\" in s.columns else (\"lat\" if \"lat\" in s.columns else None)\n",
    "    lng_col  = \"station_lng\" if \"station_lng\" in s.columns else (\"lng\" if \"lng\" in s.columns else None)\n",
    "\n",
    "    # Required metrics\n",
    "    risk_pct_col = \"risk_index_pct\" if \"risk_index_pct\" in s.columns else None\n",
    "    rate_col     = \"eb_risk_rate_per_100k_trips\" if \"eb_risk_rate_per_100k_trips\" in s.columns else None\n",
    "    exposure_col = \"exposure_trips\" if \"exposure_trips\" in s.columns else (\"trips\" if \"trips\" in s.columns else None)\n",
    "    crash_col    = \"crash_count\" if \"crash_count\" in s.columns else None\n",
    "    cred_col     = \"credibility_flag\" if \"credibility_flag\" in s.columns else None\n",
    "\n",
    "    missing_required = [k for k, v in {\n",
    "        \"station_id\": id_col,\n",
    "        \"station_name\": name_col,\n",
    "        \"risk_index_pct\": risk_pct_col,\n",
    "        \"eb_risk_rate_per_100k_trips\": rate_col,\n",
    "        \"exposure_trips\": exposure_col,\n",
    "    }.items() if v is None]\n",
    "\n",
    "    if missing_required:\n",
    "        print(\"‚ö†Ô∏è  Cannot run high-risk analysis; missing columns:\", missing_required)\n",
    "        print(\"Available columns:\", list(s.columns))\n",
    "    else:\n",
    "        # ---- Clean numeric columns ----\n",
    "        s[risk_pct_col] = pd.to_numeric(s[risk_pct_col], errors=\"coerce\")\n",
    "        s[rate_col]     = pd.to_numeric(s[rate_col], errors=\"coerce\")\n",
    "        s[exposure_col] = pd.to_numeric(s[exposure_col], errors=\"coerce\")\n",
    "\n",
    "        if crash_col is not None:\n",
    "            s[crash_col] = pd.to_numeric(s[crash_col], errors=\"coerce\")\n",
    "\n",
    "        # ---- Credible filter (if available) ----\n",
    "        if cred_col is not None:\n",
    "            credible_stations = s[s[cred_col].astype(str).str.lower() == \"credible\"].copy()\n",
    "        else:\n",
    "            credible_stations = s.copy()\n",
    "            print(\"‚ö†Ô∏è  No credibility_flag column; using all stations as 'credible_stations'.\")\n",
    "\n",
    "        credible_stations = credible_stations.dropna(subset=[risk_pct_col, rate_col, exposure_col])\n",
    "\n",
    "        if len(credible_stations) == 0:\n",
    "            print(\"‚ö†Ô∏è  No credible stations with usable risk data.\")\n",
    "        else:\n",
    "            # High-risk = >= 90th percentile\n",
    "            high_risk = credible_stations[credible_stations[risk_pct_col] >= 90].copy()\n",
    "\n",
    "            print(f\"\\nHigh-risk stations (‚â•90th percentile): {len(high_risk):,}\")\n",
    "            print(f\"Total exposure at high-risk stations: {high_risk[exposure_col].sum():,.0f} trips\")\n",
    "\n",
    "            # Top 10 by risk rate\n",
    "            cols_to_show = [id_col, name_col]\n",
    "            if lat_col: cols_to_show.append(lat_col)\n",
    "            if lng_col: cols_to_show.append(lng_col)\n",
    "            cols_to_show += [exposure_col]\n",
    "            if crash_col: cols_to_show.append(crash_col)\n",
    "            cols_to_show += [rate_col, risk_pct_col]\n",
    "\n",
    "            top_risk = (\n",
    "                high_risk.sort_values(rate_col, ascending=False)\n",
    "                        .head(10)[cols_to_show]\n",
    "                        .copy()\n",
    "            )\n",
    "\n",
    "            print(\"\\nüìç Top 10 Highest-Risk Stations:\")\n",
    "            display(top_risk)\n",
    "\n",
    "            # ---- Plot: Top 10 Highest-Risk Stations ----\n",
    "            try:\n",
    "                top_plot = top_risk.sort_values(rate_col, ascending=True).copy()\n",
    "                fig, ax = plt.subplots(figsize=(11, 6))\n",
    "\n",
    "                y = np.arange(len(top_plot))\n",
    "                ax.barh(y, top_plot[rate_col].values, edgecolor=\"black\")\n",
    "\n",
    "                ax.set_yticks(y)\n",
    "                ax.set_yticklabels([str(n)[:40] for n in top_plot[name_col]], fontsize=9)\n",
    "                ax.set_xlabel(\"E-bike Risk Rate per 100k Trips\", fontsize=11)\n",
    "                ax.set_title(\"Top 10 Highest-Risk Stations (Credible, ‚â•90th Percentile)\", fontsize=12, fontweight=\"bold\")\n",
    "                ax.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "                plt.tight_layout()\n",
    "                savefig(\"top10_highest_risk_stations.png\")\n",
    "                plt.show()\n",
    "                print(\"‚úì Saved plot: top10_highest_risk_stations.png\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Could not generate top10 plot: {e}\")\n",
    "\n",
    "            # ---- Geographic clustering (optional) ----\n",
    "            if lat_col and lng_col and len(high_risk) > 1:\n",
    "                try:\n",
    "                    from sklearn.cluster import DBSCAN\n",
    "\n",
    "                    coords = high_risk[[lat_col, lng_col]].dropna().values\n",
    "                    if len(coords) < 2:\n",
    "                        print(\"‚ö†Ô∏è  Not enough lat/lng data for clustering.\")\n",
    "                    else:\n",
    "                        clustering = DBSCAN(eps=0.01, min_samples=2).fit(coords)\n",
    "                        # align back to high_risk rows that had coords\n",
    "                        idx_valid = high_risk[[lat_col, lng_col]].dropna().index\n",
    "                        high_risk.loc[idx_valid, \"cluster\"] = clustering.labels_\n",
    "\n",
    "                        labels = clustering.labels_.tolist()\n",
    "                        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "                        n_noise = labels.count(-1)\n",
    "\n",
    "                        print(f\"\\nüó∫Ô∏è  Geographic Clustering:\")\n",
    "                        print(f\"  High-risk clusters identified: {n_clusters}\")\n",
    "                        print(f\"  Isolated high-risk stations: {n_noise}\")\n",
    "\n",
    "                        if n_clusters > 0:\n",
    "                            # cluster summary\n",
    "                            agg_map = {\n",
    "                                id_col: \"count\",\n",
    "                                exposure_col: \"sum\",\n",
    "                            }\n",
    "                            if crash_col:\n",
    "                                agg_map[crash_col] = \"sum\"\n",
    "                            agg_map[lat_col] = \"mean\"\n",
    "                            agg_map[lng_col] = \"mean\"\n",
    "\n",
    "                            cluster_summary = (\n",
    "                                high_risk[high_risk[\"cluster\"] >= 0]\n",
    "                                .groupby(\"cluster\")\n",
    "                                .agg(agg_map)\n",
    "                                .rename(columns={id_col: \"stations_in_cluster\"})\n",
    "                            )\n",
    "\n",
    "                            print(\"\\nCluster Details:\")\n",
    "                            display(cluster_summary)\n",
    "                except ImportError:\n",
    "                    print(\"‚ö†Ô∏è  sklearn not installed - skipping geographic clustering\")\n",
    "                    print(\"   Install with: pip install scikit-learn\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0327da89",
   "metadata": {},
   "source": [
    "### 8.2 Risk Distribution Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90639ef1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# --- RISK DISTRIBUTION ---\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdf_score\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(credible_stations) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRISK DISTRIBUTION ANALYSIS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_score' is not defined"
     ]
    }
   ],
   "source": [
    "# --- RISK DISTRIBUTION (robust + tier colors Low=green, Medium=yellow, High=red) ---\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if df_score is None or len(df_score) == 0:\n",
    "    print(\"‚ö†Ô∏è  df_score is None/empty (no scoring data available)\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"RISK DISTRIBUTION ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    s = df_score.copy()\n",
    "\n",
    "    # Resolve key columns\n",
    "    cred_col     = \"credibility_flag\" if \"credibility_flag\" in s.columns else None\n",
    "    risk_pct_col = \"risk_index_pct\" if \"risk_index_pct\" in s.columns else None\n",
    "    rate_col     = \"eb_risk_rate_per_100k_trips\" if \"eb_risk_rate_per_100k_trips\" in s.columns else None\n",
    "    exposure_col = \"exposure_trips\" if \"exposure_trips\" in s.columns else (\"trips\" if \"trips\" in s.columns else None)\n",
    "    crash_col    = \"crash_count\" if \"crash_count\" in s.columns else None\n",
    "    exp_inc_col  = \"expected_incidents_proxy\" if \"expected_incidents_proxy\" in s.columns else None\n",
    "\n",
    "    # station name (for plot 4)\n",
    "    name_col = \"start_station_name\" if \"start_station_name\" in s.columns else (\"station_name\" if \"station_name\" in s.columns else None)\n",
    "\n",
    "    # Basic requirements for most plots\n",
    "    if rate_col is None or exposure_col is None:\n",
    "        print(\"‚ö†Ô∏è  Missing required columns for distribution plots.\")\n",
    "        print(\"Need at least:\", [\"eb_risk_rate_per_100k_trips\", \"exposure_trips/trips\"])\n",
    "        print(\"Available columns:\", list(s.columns))\n",
    "    else:\n",
    "        # Clean numeric\n",
    "        s[rate_col] = pd.to_numeric(s[rate_col], errors=\"coerce\")\n",
    "        s[exposure_col] = pd.to_numeric(s[exposure_col], errors=\"coerce\")\n",
    "        if crash_col is not None:\n",
    "            s[crash_col] = pd.to_numeric(s[crash_col], errors=\"coerce\")\n",
    "        if risk_pct_col is not None:\n",
    "            s[risk_pct_col] = pd.to_numeric(s[risk_pct_col], errors=\"coerce\")\n",
    "        if exp_inc_col is not None:\n",
    "            s[exp_inc_col] = pd.to_numeric(s[exp_inc_col], errors=\"coerce\")\n",
    "\n",
    "        # Credible filter\n",
    "        if cred_col is not None:\n",
    "            credible_stations = s[s[cred_col].astype(str).str.lower() == \"credible\"].copy()\n",
    "        else:\n",
    "            credible_stations = s.copy()\n",
    "            print(\"‚ö†Ô∏è  No credibility_flag column; using all stations as 'credible_stations'.\")\n",
    "\n",
    "        credible_stations = credible_stations.dropna(subset=[rate_col, exposure_col])\n",
    "\n",
    "        if len(credible_stations) == 0:\n",
    "            print(\"‚ö†Ô∏è  No credible stations with usable risk+exposure data.\")\n",
    "        else:\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "            # 1) Risk Histogram\n",
    "            ax = axes[0, 0]\n",
    "            vals = credible_stations[rate_col].dropna()\n",
    "            ax.hist(vals, bins=30, edgecolor=\"black\", alpha=0.7)\n",
    "            ax.axvline(vals.median(), linestyle=\"--\", linewidth=2, label=\"Median\")\n",
    "            ax.set_xlabel(\"EB Risk Rate (per 100k trips)\", fontsize=11)\n",
    "            ax.set_ylabel(\"Number of Stations\", fontsize=11)\n",
    "            ax.set_title(\"Risk Distribution (Credible Stations Only)\", fontsize=12, fontweight=\"bold\")\n",
    "            ax.legend()\n",
    "            ax.grid(alpha=0.3)\n",
    "\n",
    "            # 2) Exposure vs Risk Scatter (color by crash_count if available)\n",
    "            ax = axes[0, 1]\n",
    "            if crash_col is not None:\n",
    "                scatter = ax.scatter(\n",
    "                    credible_stations[exposure_col],\n",
    "                    credible_stations[rate_col],\n",
    "                    c=credible_stations[crash_col].fillna(0),\n",
    "                    alpha=0.6,\n",
    "                    s=50\n",
    "                )\n",
    "                plt.colorbar(scatter, ax=ax, label=\"Crash Count\")\n",
    "            else:\n",
    "                ax.scatter(\n",
    "                    credible_stations[exposure_col],\n",
    "                    credible_stations[rate_col],\n",
    "                    alpha=0.6,\n",
    "                    s=50\n",
    "                )\n",
    "            ax.set_xlabel(\"Exposure (trips)\", fontsize=11)\n",
    "            ax.set_ylabel(\"EB Risk Rate (per 100k trips)\", fontsize=11)\n",
    "            ax.set_title(\"Risk vs Exposure\", fontsize=12, fontweight=\"bold\")\n",
    "            ax.set_xscale(\"log\")\n",
    "            ax.grid(alpha=0.3)\n",
    "\n",
    "            # Mark prevention hotspots if present\n",
    "            if \"prevention_hotspot\" in credible_stations.columns:\n",
    "                hotspots = credible_stations[credible_stations[\"prevention_hotspot\"] == True]\n",
    "                if len(hotspots) > 0:\n",
    "                    ax.scatter(\n",
    "                        hotspots[exposure_col],\n",
    "                        hotspots[rate_col],\n",
    "                        color=\"red\", s=200, marker=\"*\",\n",
    "                        edgecolors=\"black\", linewidths=1,\n",
    "                        label=\"Prevention Hotspots\", zorder=5\n",
    "                    )\n",
    "                    ax.legend()\n",
    "\n",
    "            # 3) Risk Tiers (Low green, Medium yellow, High red)\n",
    "            ax = axes[1, 0]\n",
    "            if risk_pct_col is not None:\n",
    "                tmp = credible_stations.dropna(subset=[risk_pct_col]).copy()\n",
    "                if len(tmp) > 0:\n",
    "                    tmp[\"risk_tier\"] = pd.cut(\n",
    "                        tmp[risk_pct_col],\n",
    "                        bins=[0, 33, 66, 100],\n",
    "                        labels=[\"Low Risk\", \"Medium Risk\", \"High Risk\"],\n",
    "                        include_lowest=True\n",
    "                    )\n",
    "\n",
    "                    # IMPORTANT: enforce consistent order\n",
    "                    order = [\"Low Risk\", \"Medium Risk\", \"High Risk\"]\n",
    "                    tier_counts = tmp[\"risk_tier\"].value_counts().reindex(order).fillna(0).astype(int)\n",
    "\n",
    "                    # Colors you asked:\n",
    "                    tier_colors = {\n",
    "                        \"Low Risk\": \"green\",\n",
    "                        \"Medium Risk\": \"yellow\",\n",
    "                        \"High Risk\": \"red\"\n",
    "                    }\n",
    "\n",
    "                    ax.bar(\n",
    "                        tier_counts.index.tolist(),\n",
    "                        tier_counts.values,\n",
    "                        edgecolor=\"black\",\n",
    "                        color=[tier_colors[k] for k in tier_counts.index.tolist()]\n",
    "                    )\n",
    "\n",
    "                    ax.set_xlabel(\"Risk Tier\", fontsize=11)\n",
    "                    ax.set_ylabel(\"Number of Stations\", fontsize=11)\n",
    "                    ax.set_title(\"Station Distribution by Risk Tier\", fontsize=12, fontweight=\"bold\")\n",
    "                    ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
    "                    ax.grid(alpha=0.3, axis=\"y\")\n",
    "\n",
    "                    # Add percentages on bars\n",
    "                    denom = len(tmp)\n",
    "                    for i, v in enumerate(tier_counts.values):\n",
    "                        pct = (v / denom * 100) if denom > 0 else 0\n",
    "                        ax.text(i, v, f\"{v}\\n({pct:.1f}%)\", ha=\"center\", va=\"bottom\", fontweight=\"bold\")\n",
    "                else:\n",
    "                    ax.set_title(\"Station Distribution by Risk Tier (no data)\", fontsize=12, fontweight=\"bold\")\n",
    "                    ax.axis(\"off\")\n",
    "            else:\n",
    "                ax.set_title(\"Station Distribution by Risk Tier (missing risk_index_pct)\", fontsize=12, fontweight=\"bold\")\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "            # 4) Expected Incidents Proxy (Top 20) if present\n",
    "            ax = axes[1, 1]\n",
    "            if exp_inc_col is not None and name_col is not None:\n",
    "                top_20 = credible_stations.dropna(subset=[exp_inc_col]).nlargest(20, exp_inc_col).copy()\n",
    "                if len(top_20) > 0:\n",
    "                    ax.barh(range(len(top_20)), top_20[exp_inc_col].values, edgecolor=\"black\")\n",
    "                    ax.set_yticks(range(len(top_20)))\n",
    "                    labels = []\n",
    "                    for n in top_20[name_col].astype(str).tolist():\n",
    "                        labels.append((n[:30] + \"...\") if len(n) > 30 else n)\n",
    "                    ax.set_yticklabels(labels, fontsize=8)\n",
    "                    ax.set_xlabel(\"Expected Incidents Proxy\", fontsize=11)\n",
    "                    ax.set_title(\"Top 20 Stations by Expected Impact\", fontsize=12, fontweight=\"bold\")\n",
    "                    ax.grid(alpha=0.3, axis=\"x\")\n",
    "                else:\n",
    "                    ax.set_title(\"Top 20 Stations by Expected Impact (no data)\", fontsize=12, fontweight=\"bold\")\n",
    "                    ax.axis(\"off\")\n",
    "            else:\n",
    "                ax.set_title(\"Top 20 Stations by Expected Impact (missing column)\", fontsize=12, fontweight=\"bold\")\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            savefig(\"08_risk_distribution_analysis.png\")\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f431626",
   "metadata": {},
   "source": [
    "### 8.3 Temporal Risk Patterns: WHEN does risk peak?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31b78e88",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_hour' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# --- TEMPORAL PATTERNS: HOUR OF DAY ---\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdf_hour\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTEMPORAL PATTERNS: HOUR OF DAY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_hour' is not defined"
     ]
    }
   ],
   "source": [
    "# --- TEMPORAL PATTERNS: HOUR OF DAY ---\n",
    "if df_hour is not None:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEMPORAL PATTERNS: HOUR OF DAY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Hour data is already aggregated system-wide\n",
    "    hour_summary = df_hour.groupby('hour')['trips'].sum().reset_index()\n",
    "    hour_summary = hour_summary.sort_values('hour')\n",
    "    \n",
    "    # Calculate percentage of daily trips\n",
    "    total_trips = hour_summary['trips'].sum()\n",
    "    hour_summary['pct_of_daily'] = (hour_summary['trips'] / total_trips) * 100\n",
    "    \n",
    "    # Plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # 1. Trip volume by hour\n",
    "    ax1.bar(hour_summary['hour'], hour_summary['trips'], \n",
    "           color='#2E86AB', edgecolor='black', alpha=0.7)\n",
    "    ax1.set_xlabel('Hour of Day', fontsize=12)\n",
    "    ax1.set_ylabel('Total Trips', fontsize=12)\n",
    "    ax1.set_title(f'Trip Volume by Hour ‚Äî mode={mode}', fontsize=13, fontweight='bold')\n",
    "    ax1.set_xticks(range(0, 24))\n",
    "    ax1.grid(alpha=0.3, axis='y')\n",
    "    \n",
    "    # Mark rush hours\n",
    "    rush_hours = [8, 9, 17, 18]\n",
    "    for rh in rush_hours:\n",
    "        ax1.axvline(rh, color='orange', linestyle='--', alpha=0.6, linewidth=2)\n",
    "    \n",
    "    # 2. Percentage distribution\n",
    "    ax2.plot(hour_summary['hour'], hour_summary['pct_of_daily'], \n",
    "            marker='o', linewidth=2, markersize=8, color='#E74C3C')\n",
    "    ax2.fill_between(hour_summary['hour'], hour_summary['pct_of_daily'], \n",
    "                     alpha=0.3, color='#E74C3C')\n",
    "    ax2.set_xlabel('Hour of Day', fontsize=12)\n",
    "    ax2.set_ylabel('% of Daily Trips', fontsize=12)\n",
    "    ax2.set_title('Distribution of Trips Throughout Day', fontsize=13, fontweight='bold')\n",
    "    ax2.set_xticks(range(0, 24))\n",
    "    ax2.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    savefig(\"09_trips_by_hour.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìä Hour-of-Day Analysis:\")\n",
    "    top_hours = hour_summary.nlargest(5, 'trips')[['hour', 'trips', 'pct_of_daily']]\n",
    "    print(\"\\nTop 5 Busiest Hours:\")\n",
    "    display(top_hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1956d61",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_dow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# --- TEMPORAL PATTERNS: DAY OF WEEK ---\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdf_dow\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTEMPORAL PATTERNS: DAY OF WEEK\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_dow' is not defined"
     ]
    }
   ],
   "source": [
    "# --- TEMPORAL PATTERNS: DAY OF WEEK (robust) ---\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if df_dow is None:\n",
    "    print(\"df_dow is None; skipping DOW section.\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEMPORAL PATTERNS: DAY OF WEEK\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    d = df_dow.copy()\n",
    "\n",
    "    if \"dow\" not in d.columns or \"trips\" not in d.columns:\n",
    "        raise KeyError(f\"df_dow missing required columns. Have: {list(d.columns)}\")\n",
    "\n",
    "    d[\"dow\"] = pd.to_numeric(d[\"dow\"], errors=\"coerce\")\n",
    "    d[\"trips\"] = pd.to_numeric(d[\"trips\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    d = d.dropna(subset=[\"dow\"]).copy()\n",
    "    d[\"dow\"] = d[\"dow\"].astype(int)\n",
    "\n",
    "    if \"dow_name\" not in d.columns:\n",
    "        dow_map = {0:\"Monday\",1:\"Tuesday\",2:\"Wednesday\",3:\"Thursday\",4:\"Friday\",5:\"Saturday\",6:\"Sunday\"}\n",
    "        d[\"dow_name\"] = d[\"dow\"].map(dow_map).fillna(d[\"dow\"].astype(str))\n",
    "\n",
    "    if \"week_part\" not in d.columns:\n",
    "        d[\"week_part\"] = np.where(d[\"dow\"] >= 5, \"weekend\", \"weekday\")\n",
    "\n",
    "    dow_summary = (\n",
    "        d.groupby([\"dow\", \"dow_name\"], as_index=False)[\"trips\"]\n",
    "         .sum()\n",
    "         .sort_values(\"dow\")\n",
    "    )\n",
    "\n",
    "    total_trips = dow_summary[\"trips\"].sum()\n",
    "    dow_summary[\"pct_of_weekly\"] = (dow_summary[\"trips\"] / total_trips) * 100 if total_trips > 0 else 0.0\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    colors = [\"#4CAF50\" if x in (\"Saturday\",\"Sunday\") else \"#2E86AB\" for x in dow_summary[\"dow_name\"]]\n",
    "    ax1.bar(dow_summary[\"dow_name\"], dow_summary[\"trips\"], color=colors, edgecolor=\"black\", alpha=0.7)\n",
    "    ax1.set_xlabel(\"Day of Week\", fontsize=11)\n",
    "    ax1.set_ylabel(\"Total Trips\", fontsize=11)\n",
    "    ax1.set_title(f\"Trip Volume by Day of Week ‚Äî mode={mode}\", fontsize=12, fontweight=\"bold\")\n",
    "    ax1.tick_params(axis=\"x\", rotation=45)\n",
    "    ax1.grid(alpha=0.3, axis=\"y\")\n",
    "\n",
    "    for i, row in dow_summary.reset_index(drop=True).iterrows():\n",
    "        ax1.text(i, row[\"trips\"], f\"{row['trips']:,.0f}\",\n",
    "                 ha=\"center\", va=\"bottom\", fontweight=\"bold\", fontsize=9)\n",
    "\n",
    "    wp = d.groupby(\"week_part\", as_index=False)[\"trips\"].sum()\n",
    "    wp[\"pct\"] = (wp[\"trips\"] / wp[\"trips\"].sum()) * 100 if wp[\"trips\"].sum() > 0 else 0.0\n",
    "    colors_wp = [\"#2E86AB\" if x == \"weekday\" else \"#4CAF50\" for x in wp[\"week_part\"]]\n",
    "    ax2.bar(wp[\"week_part\"], wp[\"trips\"], color=colors_wp, edgecolor=\"black\", width=0.6, alpha=0.7)\n",
    "    ax2.set_xlabel(\"Week Part\", fontsize=11)\n",
    "    ax2.set_ylabel(\"Total Trips\", fontsize=11)\n",
    "    ax2.set_title(\"Weekday vs Weekend\", fontsize=12, fontweight=\"bold\")\n",
    "    ax2.grid(alpha=0.3, axis=\"y\")\n",
    "\n",
    "    for i, row in wp.reset_index(drop=True).iterrows():\n",
    "        ax2.text(i, row[\"trips\"], f\"{row['trips']:,.0f}\\n({row['pct']:.1f}%)\",\n",
    "                 ha=\"center\", va=\"bottom\", fontweight=\"bold\", fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    savefig(\"10_trips_by_day_of_week.png\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nüìä Day-of-Week Analysis:\")\n",
    "    display(dow_summary[[\"dow_name\", \"trips\", \"pct_of_weekly\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8abc4e",
   "metadata": {},
   "source": [
    "### 8.4 Executive Risk Summary for AXA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "338d4bbb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# --- EXECUTIVE RISK SUMMARY ---\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdf_score\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEXECUTIVE RISK SUMMARY FOR AXA\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_score' is not defined"
     ]
    }
   ],
   "source": [
    "# --- EXECUTIVE RISK SUMMARY ---\n",
    "if df_score is not None:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXECUTIVE RISK SUMMARY FOR AXA\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Create summary metrics\n",
    "    summary_data = []\n",
    "    \n",
    "    # Overall metrics\n",
    "    total_stations = len(df_score)\n",
    "    credible_count = len(df_score[df_score['credibility_flag'] == 'credible'])\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Metric': 'Total Stations',\n",
    "        'Value': f\"{total_stations:,}\",\n",
    "        'Notes': f\"{mode.upper()} mode\"\n",
    "    })\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Metric': 'Credible Stations (‚â•5K trips)',\n",
    "        'Value': f\"{credible_count:,} ({credible_count/total_stations*100:.1f}%)\",\n",
    "        'Notes': 'Suitable for risk-based pricing'\n",
    "    })\n",
    "    \n",
    "    # Risk tiers\n",
    "    if 'risk_tier' in credible_stations.columns:\n",
    "        high_risk_count = len(credible_stations[credible_stations['risk_tier'] == 'High Risk'])\n",
    "        medium_risk_count = len(credible_stations[credible_stations['risk_tier'] == 'Medium Risk'])\n",
    "        low_risk_count = len(credible_stations[credible_stations['risk_tier'] == 'Low Risk'])\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Metric': 'High Risk Stations',\n",
    "            'Value': f\"{high_risk_count:,} ({high_risk_count/credible_count*100:.1f}%)\",\n",
    "            'Notes': 'Premium pricing tier'\n",
    "        })\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Metric': 'Medium Risk Stations',\n",
    "            'Value': f\"{medium_risk_count:,} ({medium_risk_count/credible_count*100:.1f}%)\",\n",
    "            'Notes': 'Standard pricing tier'\n",
    "        })\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Metric': 'Low Risk Stations',\n",
    "            'Value': f\"{low_risk_count:,} ({low_risk_count/credible_count*100:.1f}%)\",\n",
    "            'Notes': 'Discount pricing tier'\n",
    "        })\n",
    "    \n",
    "    # Hotspots\n",
    "    if 'prevention_hotspot' in df_score.columns:\n",
    "        prevention_count = df_score['prevention_hotspot'].sum()\n",
    "        summary_data.append({\n",
    "            'Metric': 'Prevention Hotspots',\n",
    "            'Value': f\"{int(prevention_count):,}\",\n",
    "            'Notes': 'High exposure + High risk ‚Üí Safety campaigns'\n",
    "        })\n",
    "    \n",
    "    if 'product_hotspot' in df_score.columns:\n",
    "        product_count = df_score['product_hotspot'].sum()\n",
    "        summary_data.append({\n",
    "            'Metric': 'Product Hotspots',\n",
    "            'Value': f\"{int(product_count):,}\",\n",
    "            'Notes': 'High exposure ‚Üí Insurance sales opportunity'\n",
    "        })\n",
    "    \n",
    "    if 'acquisition_hotspot' in df_score.columns:\n",
    "        acquisition_count = df_score['acquisition_hotspot'].sum()\n",
    "        summary_data.append({\n",
    "            'Metric': 'Acquisition Hotspots',\n",
    "            'Value': f\"{int(acquisition_count):,}\",\n",
    "            'Notes': 'High exposure + Low risk ‚Üí Easy wins'\n",
    "        })\n",
    "    \n",
    "    # Display summary table\n",
    "    df_summary = pd.DataFrame(summary_data)\n",
    "    display(df_summary)\n",
    "    \n",
    "    # Save to CSV\n",
    "    summary_csv = RUN_DIR / \"risk_executive_summary.csv\"\n",
    "    df_summary.to_csv(summary_csv, index=False)\n",
    "    print(f\"\\n‚úÖ Saved executive summary to: {summary_csv}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "modified_at": "2026-01-03T11:12:10.237164Z",
  "modified_by": "chatgpt"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
