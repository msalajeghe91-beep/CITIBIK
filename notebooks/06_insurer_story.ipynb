{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8edcd816",
   "metadata": {},
   "source": [
    "# Citi Bike × NYPD: Usage patterns + insurer decision assets\n",
    "\n",
    "This notebook is built to work with the repo Make targets (e.g. `make report MODE=nyc YEARS=\"2023 2024\" MONTHS=\"1 2 3\"`).\n",
    "\n",
    "It **always prefers the current run** (your `summaries/<RUN_TAG>/` folder) when you pass `YEARS/MONTHS`, so the report reflects exactly what you ingested.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1edc51",
   "metadata": {},
   "source": [
    "## How to run (recommended)\n",
    "\n",
    "From the repo root:\n",
    "\n",
    "```bash\n",
    "make all-both YEARS=\"YYYY YYYY...\" MONTHS=\"1 2 .. 12\" PURGE_OLD_ZIPS=NO \n",
    "check README\n",
    "```\n",
    "\n",
    "The Makefile sets environment variables (e.g. `CITIBIKE_PARQUET_DIR`, `CITIBIKE_YEARS`, `CITIBIKE_MONTHS`) which this notebook reads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9f74bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup (STRICT): locate repo, resolve RUN_DIR + PARQUET_DIR, load summaries safely ---\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    from IPython.display import display, Markdown\n",
    "    _HAVE_IPYTHON = True\n",
    "except Exception:\n",
    "    display = print\n",
    "    Markdown = lambda x: x\n",
    "    _HAVE_IPYTHON = False\n",
    "\n",
    "# Ensure figures render into the executed notebook outputs\n",
    "try:\n",
    "    get_ipython().run_line_magic(\"matplotlib\", \"inline\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "plt.ioff()  # nbconvert-friendly (prevents interactive windows)\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    start = start.resolve()\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"Makefile\").exists():\n",
    "            return p\n",
    "    return start\n",
    "\n",
    "REPO_ROOT = find_repo_root(Path.cwd())\n",
    "SUMMARIES_ROOT = REPO_ROOT / \"summaries\"\n",
    "\n",
    "def _parse_int_list(val: str | None):\n",
    "    if val is None:\n",
    "        return None\n",
    "    s = str(val).strip()\n",
    "    if not s:\n",
    "        return None\n",
    "    parts = re.split(r\"[,\\s]+\", s)\n",
    "    out = []\n",
    "    for p in parts:\n",
    "        if not p:\n",
    "            continue\n",
    "        try:\n",
    "            out.append(int(p))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return out or None\n",
    "\n",
    "# Inputs from Makefile (recommended)\n",
    "PARQUET_DIR_ENV = (os.environ.get(\"CITIBIKE_PARQUET_DIR\") or \"\").strip()\n",
    "RUN_DIR_ENV     = (os.environ.get(\"CITIBIKE_RUN_DIR\") or \"\").strip()\n",
    "MODE_ENV        = (os.environ.get(\"CITIBIKE_MODE\") or os.environ.get(\"MODE\") or \"\").strip().lower()\n",
    "\n",
    "YEARS_FILTER  = _parse_int_list(os.environ.get(\"CITIBIKE_YEARS\")  or os.environ.get(\"YEARS\"))\n",
    "MONTHS_FILTER = _parse_int_list(os.environ.get(\"CITIBIKE_MONTHS\") or os.environ.get(\"MONTHS\"))\n",
    "\n",
    "PARQUET_DIR = Path(PARQUET_DIR_ENV) if PARQUET_DIR_ENV else Path()\n",
    "if RUN_DIR_ENV:\n",
    "    RUN_DIR = Path(RUN_DIR_ENV)\n",
    "else:\n",
    "    # Infer RUN_TAG from parquet folder name; expects .../citibike_parquet/<RUN_TAG>\n",
    "    run_tag = PARQUET_DIR.name if str(PARQUET_DIR).strip() else \"\"\n",
    "    RUN_DIR = SUMMARIES_ROOT / run_tag if run_tag else Path()\n",
    "\n",
    "# Resolve relative -> absolute (relative to REPO_ROOT)\n",
    "if RUN_DIR and (not RUN_DIR.is_absolute()):\n",
    "    RUN_DIR = (REPO_ROOT / RUN_DIR).resolve()\n",
    "if PARQUET_DIR and (not PARQUET_DIR.is_absolute()):\n",
    "    PARQUET_DIR = (REPO_ROOT / PARQUET_DIR).resolve()\n",
    "\n",
    "# ---- STRICT checks ----\n",
    "if not (REPO_ROOT / \"Makefile\").exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Expected to find repo root (Makefile) by walking up from CWD.\\n\"\n",
    "        f\"  CWD: {Path.cwd().resolve()}\\n\"\n",
    "        f\"  Derived REPO_ROOT: {REPO_ROOT}\"\n",
    "    )\n",
    "\n",
    "if not SUMMARIES_ROOT.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Expected summaries/ folder not found at: {SUMMARIES_ROOT}\\n\"\n",
    "        \"Run: make summarize (or make all) first.\"\n",
    "    )\n",
    "\n",
    "if not RUN_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Expected summaries run_dir not found:\\n  {RUN_DIR}\\n\"\n",
    "        \"Run: make summarize (or make all) first.\"\n",
    "    )\n",
    "\n",
    "if not PARQUET_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Expected parquet dir not found:\\n  {PARQUET_DIR}\\n\"\n",
    "        \"Run: make ingest (or make all) first.\"\n",
    "    )\n",
    "\n",
    "REQUIRED_RUN_FILES = [\n",
    "    \"citibike_trips_by_year.csv\",\n",
    "    \"citibike_trips_by_month.csv\",\n",
    "    \"citibike_trips_by_dow.csv\",\n",
    "    \"citibike_trips_by_hour.csv\",\n",
    "]\n",
    "missing = [f for f in REQUIRED_RUN_FILES if not (RUN_DIR / f).exists()]\n",
    "if missing:\n",
    "    raise FileNotFoundError(\n",
    "        \"Missing required summary CSVs in run_dir:\\n\"\n",
    "        f\"  {RUN_DIR}\\n\"\n",
    "        f\"Missing: {missing}\\n\"\n",
    "        \"Run: make summarize (or make all) first.\"\n",
    "    )\n",
    "\n",
    "print(\"REPO_ROOT:\", REPO_ROOT)\n",
    "print(\"PARQUET_DIR:\", PARQUET_DIR)\n",
    "print(\"RUN_DIR:\", RUN_DIR)\n",
    "print(\"MODE (env):\", MODE_ENV or \"(not set)\")\n",
    "print(\"YEARS_FILTER:\", YEARS_FILTER, \"MONTHS_FILTER:\", MONTHS_FILTER)\n",
    "\n",
    "# Where to save figures (also show inline)\n",
    "FIG_DIR = REPO_ROOT / \"reports\" / RUN_DIR.name / \"figures\"\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"FIG_DIR:\", FIG_DIR)\n",
    "\n",
    "def savefig(filename: str):\n",
    "    out = FIG_DIR / filename\n",
    "    plt.savefig(out, dpi=200, bbox_inches=\"tight\")\n",
    "    print(\"Saved:\", out)\n",
    "\n",
    "def read_csv(path: Path) -> pd.DataFrame:\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "# ---- Load per-run summary CSVs ----\n",
    "df_year  = read_csv(RUN_DIR / \"citibike_trips_by_year.csv\")\n",
    "df_month = read_csv(RUN_DIR / \"citibike_trips_by_month.csv\")\n",
    "df_dow   = read_csv(RUN_DIR / \"citibike_trips_by_dow.csv\")\n",
    "df_hour  = read_csv(RUN_DIR / \"citibike_trips_by_hour.csv\")\n",
    "\n",
    "# Optional per-run outputs\n",
    "station_path = RUN_DIR / \"citibike_station_exposure.csv\"\n",
    "df_station = read_csv(station_path) if station_path.exists() else None\n",
    "\n",
    "risk_path = RUN_DIR / \"station_risk_exposure_plus_crashproximity.csv\"\n",
    "df_risk = read_csv(risk_path) if risk_path.exists() else None\n",
    "\n",
    "highlights_path = RUN_DIR / \"summary_highlights.md\"\n",
    "\n",
    "# Mode detection (prefer data, else env)\n",
    "mode = (\n",
    "    str(df_year[\"mode\"].iloc[0]).lower()\n",
    "    if (\"mode\" in df_year.columns and len(df_year))\n",
    "    else (MODE_ENV or \"unknown\")\n",
    ")\n",
    "print(\"Detected mode:\", mode)\n",
    "\n",
    "# ---- Compare tables (optional) ----\n",
    "# If YEARS/MONTHS filters are passed (e.g., Jan–Mar), we default to THIS run only.\n",
    "USE_COMPARE = (YEARS_FILTER is None and MONTHS_FILTER is None)\n",
    "COMPARE_DIR = SUMMARIES_ROOT / \"_compare\"\n",
    "\n",
    "def _maybe_load_compare(fname: str) -> pd.DataFrame | None:\n",
    "    p = COMPARE_DIR / fname\n",
    "    return read_csv(p) if p.exists() else None\n",
    "\n",
    "if USE_COMPARE and COMPARE_DIR.exists():\n",
    "    df_year_all  = _maybe_load_compare(\"citibike_trips_by_year_ALL.csv\")  or df_year.copy()\n",
    "    df_month_all = _maybe_load_compare(\"citibike_trips_by_month_ALL.csv\") or df_month.copy()\n",
    "    df_dow_all   = _maybe_load_compare(\"citibike_trips_by_dow_ALL.csv\")   or df_dow.copy()\n",
    "    df_hour_all  = _maybe_load_compare(\"citibike_trips_by_hour_ALL.csv\")  or df_hour.copy()\n",
    "else:\n",
    "    df_year_all, df_month_all, df_dow_all, df_hour_all = df_year.copy(), df_month.copy(), df_dow.copy(), df_hour.copy()\n",
    "\n",
    "# Always filter _ALL tables to current mode (if they have a mode column)\n",
    "for _name in [\"df_year_all\", \"df_month_all\", \"df_dow_all\", \"df_hour_all\"]:\n",
    "    _df = locals()[_name]\n",
    "    if isinstance(_df, pd.DataFrame) and (\"mode\" in _df.columns):\n",
    "        locals()[_name] = _df[_df[\"mode\"].astype(str).str.lower() == mode].copy()\n",
    "\n",
    "# If filters are provided, enforce them on the per-run tables too (defensive)\n",
    "def _filter_year_month(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    if YEARS_FILTER is not None and \"year\" in out.columns:\n",
    "        out[\"year\"] = pd.to_numeric(out[\"year\"], errors=\"coerce\")\n",
    "        out = out[out[\"year\"].isin(YEARS_FILTER)]\n",
    "    if MONTHS_FILTER is not None and \"month\" in out.columns:\n",
    "        out[\"month\"] = pd.to_numeric(out[\"month\"], errors=\"coerce\")\n",
    "        out = out[out[\"month\"].isin(MONTHS_FILTER)]\n",
    "    return out\n",
    "\n",
    "df_year  = _filter_year_month(df_year)\n",
    "df_month = _filter_year_month(df_month)\n",
    "df_dow   = _filter_year_month(df_dow)\n",
    "df_hour  = _filter_year_month(df_hour)\n",
    "\n",
    "# Helpful run label for titles\n",
    "run_label = RUN_DIR.name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89aad0e",
   "metadata": {},
   "source": [
    "## Executive summary (optional)\n",
    "\n",
    "If you generated `summary_highlights.md` for this run, we show it here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566454aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Quick text highlights from summarize script (if present) ---\n",
    "if highlights_path.exists():\n",
    "    txt = highlights_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    display(Markdown(txt))\n",
    "else:\n",
    "    print(\"No summary_highlights.md found at:\", highlights_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd163137",
   "metadata": {},
   "source": [
    "## 1) Yearly usage (comparison across years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aa930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Trips by year (THIS RUN) ---\n",
    "g = df_year.copy()\n",
    "\n",
    "if \"year\" in g.columns:\n",
    "    # Clean and prepare data\n",
    "    g[\"year\"] = pd.to_numeric(g[\"year\"], errors=\"coerce\")\n",
    "    g = g.dropna(subset=[\"year\"]).sort_values(\"year\").reset_index(drop=True)\n",
    "    \n",
    "display(g)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "if \"year\" in g.columns and \"trips\" in g.columns:\n",
    "    # Extract values as plain lists (no pandas magic)\n",
    "    years = [int(y) for y in g[\"year\"]]\n",
    "    trips = [int(t) for t in g[\"trips\"]]\n",
    "    \n",
    "    # Plot with explicit year labels\n",
    "    plt.plot(years, trips, marker=\"o\", linewidth=2, markersize=8, color='#2E86AB')\n",
    "    plt.xticks(years, [str(y) for y in years])  # Explicit year labels\n",
    "    plt.grid(True, alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Add value labels on points\n",
    "    for year, trip in zip(years, trips):\n",
    "        plt.text(year, trip, f'{trip/1e6:.1f}M', \n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.title(f\"Trips by year — mode={mode} ({run_label})\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Year\", fontsize=12)\n",
    "plt.ylabel(\"Trips\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "savefig(\"01_trips_by_year.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d53a71c",
   "metadata": {},
   "source": [
    "## 2) Month patterns (comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0391966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Trips by month (THIS RUN) ---\n",
    "m = df_month.copy()\n",
    "\n",
    "# Normalize month labels\n",
    "if \"month\" in m.columns:\n",
    "    m[\"month\"] = pd.to_numeric(m[\"month\"], errors=\"coerce\").astype(\"Int64\")\n",
    "if \"year\" in m.columns:\n",
    "    m[\"year\"] = pd.to_numeric(m[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "display(m.sort_values([c for c in [\"year\", \"month\"] if c in m.columns]))\n",
    "\n",
    "# If multiple years, show stacked-ish bars by year-month\n",
    "if {\"year\", \"month\", \"trips\"}.issubset(m.columns):\n",
    "    m2 = m.dropna(subset=[\"year\",\"month\"]).copy()\n",
    "    m2[\"ym\"] = m2[\"year\"].astype(int).astype(str) + \"-\" + m2[\"month\"].astype(int).astype(str).str.zfill(2)\n",
    "\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.bar(m2[\"ym\"], m2[\"trips\"])\n",
    "    plt.title(f\"Trips by year-month — mode={mode} ({run_label})\")\n",
    "    plt.xlabel(\"Year-Month\")\n",
    "    plt.ylabel(\"Trips\")\n",
    "    plt.xticks(rotation=75, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    savefig(\"02_trips_by_year_month.png\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Expected columns not present for trips-by-month plot. Have:\", list(m.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4dbcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Month-of-year seasonality (line chart per year) ---\n",
    "m = df_month.copy()\n",
    "if not {\"year\",\"month\",\"trips\"}.issubset(m.columns):\n",
    "    print(\"Month table missing required columns:\", list(m.columns))\n",
    "else:\n",
    "    m[\"year\"]  = pd.to_numeric(m[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    m[\"month\"] = pd.to_numeric(m[\"month\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    m = m.dropna(subset=[\"year\",\"month\"]).copy()\n",
    "    m = m.sort_values([\"year\",\"month\"])\n",
    "\n",
    "    pivot = m.pivot_table(index=\"month\", columns=\"year\", values=\"trips\", aggfunc=\"sum\").sort_index()\n",
    "    display(pivot)\n",
    "\n",
    "    plt.figure(figsize=(8,4))\n",
    "    for y in pivot.columns:\n",
    "        plt.plot(pivot.index.astype(int), pivot[y], marker=\"o\", label=str(int(y)))\n",
    "    plt.title(f\"Trips by month-of-year — mode={mode} ({run_label})\")\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"Trips\")\n",
    "    plt.xticks(range(1,13))\n",
    "    plt.legend(title=\"Year\", ncol=2, fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    savefig(\"03_month_by_year_lines.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3147d38",
   "metadata": {},
   "source": [
    "## 3) Day-of-week patterns (comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73463fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Trips by day-of-week (THIS RUN) ---\n",
    "d = df_dow.copy()\n",
    "display(d)\n",
    "\n",
    "# Prefer an ordered weekday axis if possible\n",
    "if \"dow\" in d.columns:\n",
    "    order = [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\n",
    "    d[\"dow\"] = d[\"dow\"].astype(str)\n",
    "    if set(order).issubset(set(d[\"dow\"].unique())):\n",
    "        d[\"dow\"] = pd.Categorical(d[\"dow\"], categories=order, ordered=True)\n",
    "        d = d.sort_values(\"dow\")\n",
    "\n",
    "if {\"dow\",\"trips\"}.issubset(d.columns):\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.bar(d[\"dow\"].astype(str), d[\"trips\"])\n",
    "    plt.title(f\"Trips by day-of-week — mode={mode} ({run_label})\")\n",
    "    plt.xlabel(\"Day of week\")\n",
    "    plt.ylabel(\"Trips\")\n",
    "    plt.xticks(rotation=30, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    savefig(\"04_trips_by_dow.png\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Day-of-week table missing required columns. Have:\", list(d.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e151ebc",
   "metadata": {},
   "source": [
    "## 4) Hour-of-day patterns (comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e801d472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Trips by hour (weekday vs weekend if present) ---\n",
    "h = df_hour.copy()\n",
    "display(h.head(20))\n",
    "\n",
    "# Expect cols: hour, trips, week_part or segment\n",
    "hour_col = \"hour\" if \"hour\" in h.columns else None\n",
    "trips_col = \"trips\" if \"trips\" in h.columns else None\n",
    "part_col = None\n",
    "for c in [\"week_part\",\"segment\",\"is_weekend\"]:\n",
    "    if c in h.columns:\n",
    "        part_col = c\n",
    "        break\n",
    "\n",
    "if not (hour_col and trips_col):\n",
    "    print(\"Hour table missing required columns. Have:\", list(h.columns))\n",
    "else:\n",
    "    h[hour_col] = pd.to_numeric(h[hour_col], errors=\"coerce\")\n",
    "    h[trips_col] = pd.to_numeric(h[trips_col], errors=\"coerce\")\n",
    "    h = h.dropna(subset=[hour_col, trips_col]).sort_values(hour_col)\n",
    "\n",
    "    if part_col:\n",
    "        # Plot each segment as its own line\n",
    "        plt.figure(figsize=(9,4))\n",
    "        for seg, sub in h.groupby(part_col):\n",
    "            sub = sub.sort_values(hour_col)\n",
    "            plt.plot(sub[hour_col], sub[trips_col], marker=\"o\", label=str(seg))\n",
    "        plt.title(f\"Trips by hour — mode={mode} ({run_label})\")\n",
    "        plt.xlabel(\"Hour\")\n",
    "        plt.ylabel(\"Trips\")\n",
    "        plt.xticks(range(0,24,1), rotation=0)\n",
    "        plt.legend(title=part_col)\n",
    "        plt.tight_layout()\n",
    "        savefig(\"05_trips_by_hour.png\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.figure(figsize=(9,4))\n",
    "        plt.plot(h[hour_col], h[trips_col], marker=\"o\")\n",
    "        plt.title(f\"Trips by hour — mode={mode} ({run_label})\")\n",
    "        plt.xlabel(\"Hour\")\n",
    "        plt.ylabel(\"Trips\")\n",
    "        plt.xticks(range(0,24,1), rotation=0)\n",
    "        plt.tight_layout()\n",
    "        savefig(\"05_trips_by_hour.png\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce32dcc",
   "metadata": {},
   "source": [
    "## 5) Station exposure (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf097631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Station exposure: where trips concentrate (policies / riders) ---\n",
    "if df_station is None:\n",
    "    print(\"No station exposure file found (citibike_station_exposure.csv). Skipping.\")\n",
    "else:\n",
    "    s = df_station.copy()\n",
    "\n",
    "    # Normalize column names across modes / scripts\n",
    "    if \"station_name\" not in s.columns and \"start_station_name\" in s.columns:\n",
    "        s = s.rename(columns={\"start_station_name\": \"station_name\"})\n",
    "    if \"station_id\" not in s.columns and \"start_station_id\" in s.columns:\n",
    "        s = s.rename(columns={\"start_station_id\": \"station_id\"})\n",
    "\n",
    "    trips_col = \"trips\" if \"trips\" in s.columns else None\n",
    "    name_col = \"station_name\" if \"station_name\" in s.columns else None\n",
    "\n",
    "    if trips_col is None or name_col is None:\n",
    "        print(\"Station exposure table missing required columns. Columns:\", list(s.columns))\n",
    "    else:\n",
    "        s[trips_col] = pd.to_numeric(s[trips_col], errors=\"coerce\")\n",
    "        s = s.dropna(subset=[trips_col])\n",
    "        top = s.sort_values(trips_col, ascending=False).head(20)\n",
    "        display(top)\n",
    "\n",
    "        plt.figure(figsize=(10,6))\n",
    "        _tmp = top.iloc[::-1]  # keep this if you want biggest on top\n",
    "        y = range(len(_tmp))\n",
    "\n",
    "        plt.barh(y, _tmp[trips_col].values)\n",
    "        plt.yticks(y, _tmp[name_col].astype(str))\n",
    "        plt.title(f\"Top stations by trips (exposure) — mode={mode}\")\n",
    "        plt.xlabel(\"Trips\")\n",
    "        plt.ylabel(\"Station\")\n",
    "        plt.tight_layout()\n",
    "        savefig(\"06_top_stations_exposure.png\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf77810",
   "metadata": {},
   "source": [
    "## 6) Crash proximity / risk proxy (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ac3431",
   "metadata": {},
   "source": [
    "### 6.1 Station-level prioritization (make the proxy usable)\n",
    "\n",
    "The raw file includes **crash counts within 250m/500m** of each station plus **trip volume**.  \n",
    "To avoid misleading outliers (e.g., stations with only a handful of trips), we:\n",
    "\n",
    "- keep IDs as **strings** (so `2231.10` doesn’t turn into `2231.1`)\n",
    "- recompute crash-rates per 100k trips (defensive)\n",
    "- rank **exposure** (most trips) separately from **risk rate** (filtered to stations with enough trips)\n",
    "\n",
    "You can tune `MIN_TRIPS_FOR_RATE` depending on how conservative you want to be.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa511a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Crash proximity risk proxy (NYC only): radius-aware + robust ---\n",
    "# This section automatically uses the same radius as the AXA scorecard run\n",
    "# (passed via Makefile as AXA_RADIUS, e.g. \"450m\").\n",
    "#\n",
    "# Why: the risk CSV only contains the radii that were computed during summarize (--radii-m).\n",
    "# So the notebook must pick a radius that exists in df_risk.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "MIN_TRIPS_FOR_RATE = 5000\n",
    "TOP_N = 20\n",
    "EB_M_PRIOR = 20000.0\n",
    "SCALE = 100000.0\n",
    "\n",
    "_RADIUS_RE = re.compile(r\"^\\s*(\\d+(?:\\.\\d+)?)\\s*(m|km)?\\s*$\", re.IGNORECASE)\n",
    "\n",
    "def parse_radius_to_m(raw: str) -> int:\n",
    "    s = (raw or \"\").strip().lower()\n",
    "    if s in (\"\", \"auto\", \"max\"):\n",
    "        return -1\n",
    "    m = _RADIUS_RE.match(s)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Bad AXA_RADIUS={raw!r}. Use like 450m, 750, 1km, auto.\")\n",
    "    val = float(m.group(1))\n",
    "    unit = (m.group(2) or \"m\").lower()\n",
    "    meters = val * (1000.0 if unit == \"km\" else 1.0)\n",
    "    if meters <= 0:\n",
    "        raise ValueError(f\"AXA_RADIUS must be > 0 (got {raw!r})\")\n",
    "    return int(round(meters))\n",
    "\n",
    "def available_radii(df: pd.DataFrame) -> list[int]:\n",
    "    radii = []\n",
    "    for c in df.columns:\n",
    "        mm = re.match(r\"^crashes_within_(\\d+)m$\", str(c))\n",
    "        if mm:\n",
    "            radii.append(int(mm.group(1)))\n",
    "    return sorted(set(radii))\n",
    "\n",
    "def ensure_int_col(df: pd.DataFrame, col: str) -> None:\n",
    "    if col not in df.columns:\n",
    "        df[col] = 0\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "def eb_rate_per_trip(k: np.ndarray, e: np.ndarray, m_prior: float) -> np.ndarray:\n",
    "    \"\"\"EB smoothing for Poisson rate per trip: (k + r0*m)/(e+m), r0=sum(k)/sum(e).\"\"\"\n",
    "    k = k.astype(float)\n",
    "    e = e.astype(float)\n",
    "    e_sum = float(np.nansum(e))\n",
    "    r0 = float(np.nansum(k) / e_sum) if e_sum > 0 else 0.0\n",
    "    return (k + r0 * m_prior) / (e + m_prior)\n",
    "\n",
    "if df_risk is None:\n",
    "    print(\"No station risk file found (station_risk_exposure_plus_crashproximity.csv). Skipping crash-proxy section.\")\n",
    "else:\n",
    "    r = df_risk.copy()\n",
    "\n",
    "    # Normalize ids/names (so downstream tables are stable)\n",
    "    if \"station_id\" not in r.columns and \"start_station_id\" in r.columns:\n",
    "        r = r.rename(columns={\"start_station_id\": \"station_id\"})\n",
    "    if \"station_name\" not in r.columns and \"start_station_name\" in r.columns:\n",
    "        r = r.rename(columns={\"start_station_name\": \"station_name\"})\n",
    "\n",
    "    # Trips cleaning\n",
    "    r[\"trips\"] = pd.to_numeric(r.get(\"trips\", pd.NA), errors=\"coerce\")\n",
    "    r = r.dropna(subset=[\"trips\"]).copy()\n",
    "    r = r[r[\"trips\"] > 0].copy()\n",
    "\n",
    "    # Choose radius: prefer AXA_RADIUS, else pick max available\n",
    "    avail = available_radii(r)\n",
    "    if not avail:\n",
    "        print(\"No crashes_within_<R>m columns found in df_risk. Skipping crash-proxy section.\")\n",
    "    else:\n",
    "        env_raw = os.environ.get(\"AXA_RADIUS\", \"auto\")\n",
    "        wanted_m = parse_radius_to_m(env_raw)\n",
    "\n",
    "        if wanted_m == -1:\n",
    "            chosen_m = max(avail)\n",
    "        else:\n",
    "            chosen_m = wanted_m\n",
    "            if chosen_m not in avail:\n",
    "                fallback = 500 if 500 in avail else max(avail)\n",
    "                print(f\"Requested radius {chosen_m}m not in df_risk. Available={avail}. Falling back to {fallback}m.\")\n",
    "                chosen_m = fallback\n",
    "\n",
    "        crash_col = f\"crashes_within_{chosen_m}m\"\n",
    "        rate_col = f\"{crash_col}_per_100k_trips\"\n",
    "\n",
    "        ensure_int_col(r, crash_col)\n",
    "\n",
    "        ## Compute canonical raw rate (keep one naming scheme)\n",
    "        #r[rate_col] = (r[crash_col] / r[\"trips\"].replace({0: np.nan})) * SCALE\n",
    "        #r[rate_col] = pd.to_numeric(r[rate_col], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "        #print(f\"Using radius={chosen_m}m -> {crash_col}, {rate_col}\")\n",
    "\n",
    "        #print(\"Diagnostic: lowest-trip stations -> raw per-100k rates explode (tiny denominator)\")\n",
    "        #display(\n",
    "        #    r.sort_values(\"trips\", ascending=True)\n",
    "        #     .head(10)[[\"mode\",\"station_id\",\"station_name\",\"trips\", crash_col, rate_col]]\n",
    "        #)\n",
    "\n",
    "        # Stable subset for ranking\n",
    "        stable = r[r[\"trips\"] >= MIN_TRIPS_FOR_RATE].copy()\n",
    "        if stable.empty:\n",
    "            print(f\"No stations with trips >= {MIN_TRIPS_FOR_RATE}. Risk ranking skipped.\")\n",
    "        else:\n",
    "            stable[\"eb_rate_per_100k_trips\"] = eb_rate_per_trip(\n",
    "                stable[crash_col].to_numpy(),\n",
    "                stable[\"trips\"].to_numpy(),\n",
    "                EB_M_PRIOR,\n",
    "            ) * SCALE\n",
    "\n",
    "            print(f\"Top {TOP_N} stations by exposure (trips) — stable only (trips ≥ {MIN_TRIPS_FOR_RATE})\")\n",
    "            display(\n",
    "                stable.sort_values(\"trips\", ascending=False)\n",
    "                      .head(TOP_N)[[\"mode\",\"station_id\",\"station_name\",\"trips\"]]\n",
    "            )\n",
    "\n",
    "            print(f\"Top {TOP_N} stations by EB-smoothed crash proxy ({chosen_m}m) — stable only (trips ≥ {MIN_TRIPS_FOR_RATE})\")\n",
    "            display(\n",
    "                stable.sort_values(\"eb_rate_per_100k_trips\", ascending=False)\n",
    "                      .head(TOP_N)[[\"mode\",\"station_id\",\"station_name\",\"trips\", crash_col, \"eb_rate_per_100k_trips\"]]\n",
    "            )\n",
    "\n",
    "            # Quick plot: exposure vs EB risk\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.scatter(stable[\"trips\"], stable[\"eb_rate_per_100k_trips\"], alpha=0.6)\n",
    "            plt.xscale(\"log\")\n",
    "            plt.xlabel(\"Trips (log scale)\")\n",
    "            plt.ylabel(f\"EB-smoothed crashes within {chosen_m}m per 100k trips (proxy)\")\n",
    "            plt.title(f\"Exposure vs EB crash-risk proxy (trips ≥ {MIN_TRIPS_FOR_RATE}) — mode={mode}\")\n",
    "            plt.tight_layout()\n",
    "            savefig(f\"09_scatter_trips_vs_EB_crash_rate_{chosen_m}m_trips_ge_{MIN_TRIPS_FOR_RATE}.png\")\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "risk_metrics_header",
   "metadata": {},
   "source": [
    "### 6.2 Alternative Risk Metrics (Intuitive Formats)\n",
    "\n",
    "The standard `per_100k_trips` metric is useful for technical analysis, but can be hard to interpret.\n",
    "This section adds alternative formats that are more intuitive for different audiences:\n",
    "\n",
    "- **Percentage** (crash_rate_pct): Good for executives\n",
    "- **Trips per crash**: Most intuitive for general public\n",
    "- **Risk level**: Classification (Very Low → Very High)\n",
    "- **Risk score**: 0-100 ranking score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7597b6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Exposure vs Risk \"Zones\" (simple quadrant view, radius-aware) ---\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "MIN_TRIPS_FOR_ZONES = 5000\n",
    "SCALE = 100000.0\n",
    "\n",
    "_RADIUS_RE = re.compile(r\"^\\s*(\\d+(?:\\.\\d+)?)\\s*(m|km)?\\s*$\", re.IGNORECASE)\n",
    "\n",
    "def parse_radius_to_m(raw: str) -> int:\n",
    "    s = (raw or \"\").strip().lower()\n",
    "    if s in (\"\", \"auto\", \"max\"):\n",
    "        return -1\n",
    "    m = _RADIUS_RE.match(s)\n",
    "    if not m:\n",
    "        return -1\n",
    "    val = float(m.group(1))\n",
    "    unit = (m.group(2) or \"m\").lower()\n",
    "    meters = val * (1000.0 if unit == \"km\" else 1.0)\n",
    "    return int(round(meters)) if meters > 0 else -1\n",
    "\n",
    "def available_radii(df: pd.DataFrame) -> list[int]:\n",
    "    radii = []\n",
    "    for c in df.columns:\n",
    "        mm = re.match(r\"^crashes_within_(\\d+)m$\", str(c))\n",
    "        if mm:\n",
    "            radii.append(int(mm.group(1)))\n",
    "    return sorted(set(radii))\n",
    "\n",
    "def ensure_int_col(df: pd.DataFrame, col: str) -> None:\n",
    "    if col not in df.columns:\n",
    "        df[col] = 0\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "if df_risk is None:\n",
    "    print(\"No station risk file found. Skipping zones plot.\")\n",
    "else:\n",
    "    r = df_risk.copy()\n",
    "    if \"station_id\" not in r.columns and \"start_station_id\" in r.columns:\n",
    "        r = r.rename(columns={\"start_station_id\": \"station_id\"})\n",
    "    if \"station_name\" not in r.columns and \"start_station_name\" in r.columns:\n",
    "        r = r.rename(columns={\"start_station_name\": \"station_name\"})\n",
    "\n",
    "    r[\"trips\"] = pd.to_numeric(r.get(\"trips\", pd.NA), errors=\"coerce\")\n",
    "    r = r.dropna(subset=[\"trips\"]).copy()\n",
    "    r = r[r[\"trips\"] > 0].copy()\n",
    "\n",
    "    avail = available_radii(r)\n",
    "    if not avail:\n",
    "        print(\"No crashes_within_<R>m columns found. Skipping zones plot.\")\n",
    "    else:\n",
    "        env_raw = os.environ.get(\"AXA_RADIUS\", \"auto\")\n",
    "        wanted = parse_radius_to_m(env_raw)\n",
    "        chosen = max(avail) if wanted == -1 else (wanted if wanted in avail else (500 if 500 in avail else max(avail)))\n",
    "\n",
    "        crash_col = f\"crashes_within_{chosen}m\"\n",
    "        ensure_int_col(r, crash_col)\n",
    "\n",
    "        r = r[(r[\"trips\"] >= MIN_TRIPS_FOR_ZONES)].copy()\n",
    "        if len(r) == 0:\n",
    "            print(\"No stations meet trips ≥\", MIN_TRIPS_FOR_ZONES, \"— skipping zones plot.\")\n",
    "        else:\n",
    "            r[\"crashes_per_100k_trips\"] = (r[crash_col] / r[\"trips\"].replace({0: np.nan})) * SCALE\n",
    "            r[\"crashes_per_100k_trips\"] = pd.to_numeric(r[\"crashes_per_100k_trips\"], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "            x = r[\"trips\"].astype(float)\n",
    "            y = r[\"crashes_per_100k_trips\"].astype(float)\n",
    "\n",
    "            x_med = float(np.nanmedian(x))\n",
    "            y_med = float(np.nanmedian(y))\n",
    "            \n",
    "            def zone(row):\n",
    "                hi_x = float(row[\"trips\"]) >= x_med\n",
    "                # FIX 1: use the radius-aware metric you computed\n",
    "                hi_y = float(row[\"crashes_per_100k_trips\"]) >= y_med\n",
    "                if hi_x and hi_y:\n",
    "                    return \"High exposure / High risk\"\n",
    "                if hi_x and (not hi_y):\n",
    "                    return \"High exposure / Lower risk\"\n",
    "                if (not hi_x) and hi_y:\n",
    "                    return \"Lower exposure / High risk\"\n",
    "                return \"Lower exposure / Lower risk\"\n",
    "            \n",
    "            r[\"zone\"] = r.apply(zone, axis=1)\n",
    "            display(r[\"zone\"].value_counts().to_frame(\"stations\"))\n",
    "\n",
    "            # Plot quadrants\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            for z, sub in r.groupby(\"zone\"):\n",
    "                # FIX 2: plot the column that exists\n",
    "                plt.scatter(sub[\"trips\"], sub[\"crashes_per_100k_trips\"], alpha=0.6, label=z)\n",
    "\n",
    "            plt.axvline(x_med, linestyle=\"--\")\n",
    "            plt.axhline(y_med, linestyle=\"--\")\n",
    "            plt.xscale(\"log\")\n",
    "            plt.xlabel(\"Trips (log scale)\")\n",
    "            plt.ylabel(f\"Crashes within {chosen}m per 100k trips (proxy)\")\n",
    "            plt.title(f\"Exposure vs risk zones — radius={chosen}m (trips ≥ {MIN_TRIPS_FOR_ZONES})\")\n",
    "            plt.legend(fontsize=8)\n",
    "            plt.tight_layout()\n",
    "            savefig(f\"10_zones_exposure_vs_risk_{chosen}m_trips_ge_{MIN_TRIPS_FOR_ZONES}.png\")\n",
    "            plt.show()\n",
    "            # FIX 3: use plt.savefig\n",
    "            #plt.savefig(f\"10_zones_exposure_vs_risk_{chosen}m_trips_ge_{MIN_TRIPS_FOR_ZONES}.png\", dpi=200)\n",
    "            #plt.show()\n",
    "\n",
    "            # Small table of \"high-high\" quadrant\n",
    "            hh = r[(x >= x_med) & (y >= y_med)].copy()\n",
    "            hh = hh.sort_values([\"crashes_per_100k_trips\", \"trips\"], ascending=False).head(15)\n",
    "            print(\"High exposure + high proxy risk (top 15):\")\n",
    "            display(hh[[\"mode\",\"station_id\",\"station_name\",\"trips\", crash_col, \"crashes_per_100k_trips\"]])\n",
    "            \n",
    "            \n",
    "            #crash_col = \"crashes_within_900m\"  # or your chosen crash_col variable\n",
    "            #print(\"non-zero stations:\", (df_risk[crash_col] > 0).sum())\n",
    "            #print(\"total crashes counted:\", int(df_risk[crash_col].sum()))\n",
    "            tmp = df_risk.copy()\n",
    "\n",
    " \n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481ad14e",
   "metadata": {},
   "source": [
    "## 7) AXA Partner Decision Assets: Where + When + What\n",
    "\n",
    "This section turns your outputs into **two decision assets**:\n",
    "\n",
    "- **WHERE to focus** (stations): `axa_partner_scorecard_500m.csv`\n",
    "- **WHEN to activate** (time windows): `axa_target_windows.csv`\n",
    "\n",
    "Both files are produced by the Make targets you already ran.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee6808c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load AXA assets for THIS run_dir (radius-aware, no global fallback) ---\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "_RADIUS_RE = re.compile(r\"^\\s*(\\d+(?:\\.\\d+)?)\\s*(m|km)?\\s*$\", re.IGNORECASE)\n",
    "\n",
    "def parse_radius_to_m(raw: str) -> int:\n",
    "    s = (raw or \"\").strip().lower()\n",
    "    if s in (\"\", \"auto\", \"max\"):\n",
    "        return -1\n",
    "    m = _RADIUS_RE.match(s)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Bad AXA_RADIUS={raw!r}. Use like 450m, 750, 1km, auto.\")\n",
    "    val = float(m.group(1))\n",
    "    unit = (m.group(2) or \"m\").lower()\n",
    "    meters = val * (1000.0 if unit == \"km\" else 1.0)\n",
    "    if meters <= 0:\n",
    "        raise ValueError(f\"AXA_RADIUS must be > 0 (got {raw!r})\")\n",
    "    return int(round(meters))\n",
    "\n",
    "def available_scorecards(run_dir: Path) -> list[int]:\n",
    "    radii = []\n",
    "    for p in run_dir.glob(\"axa_partner_scorecard_*m.csv\"):\n",
    "        mm = re.match(r\"^axa_partner_scorecard_(\\d+)m\\.csv$\", p.name)\n",
    "        if mm:\n",
    "            radii.append(int(mm.group(1)))\n",
    "    return sorted(set(radii))\n",
    "\n",
    "def scorecard_path_for_radius(run_dir: Path, radius_env: str) -> Path | None:\n",
    "    avail = available_scorecards(run_dir)\n",
    "    if not avail:\n",
    "        return None\n",
    "    wanted = parse_radius_to_m(radius_env)\n",
    "    if wanted == -1:\n",
    "        chosen = max(avail)\n",
    "    else:\n",
    "        chosen = wanted if wanted in avail else (500 if 500 in avail else max(avail))\n",
    "    p = run_dir / f\"axa_partner_scorecard_{chosen}m.csv\"\n",
    "    return p if p.exists() else None\n",
    "\n",
    "radius_env = os.environ.get(\"AXA_RADIUS\", \"auto\")\n",
    "scorecard_path = scorecard_path_for_radius(RUN_DIR, radius_env)\n",
    "\n",
    "windows_path = RUN_DIR / \"axa_target_windows.csv\"\n",
    "\n",
    "df_score = pd.read_csv(scorecard_path) if scorecard_path and scorecard_path.exists() else None\n",
    "df_windows = pd.read_csv(windows_path) if windows_path.exists() else None\n",
    "\n",
    "print(\"AXA_RADIUS (env):\", radius_env)\n",
    "print(\"Scorecard path:\", scorecard_path if scorecard_path else \"NOT FOUND\")\n",
    "print(\"Windows path:\", windows_path if windows_path.exists() else \"NOT FOUND\")\n",
    "\n",
    "print(\"df_score is None?\", df_score is None)\n",
    "print(\"df_windows is None?\", df_windows is None)\n",
    "print(\"CWD:\", os.getcwd())\n",
    "\n",
    "if df_score is not None:\n",
    "    display(df_score.head(10))\n",
    "if df_windows is not None:\n",
    "    display(df_windows.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a11461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EB-enrich the AXA scorecard (so priority behaves like an insurer) ---\n",
    "# We compute a credibility-smoothed (Empirical Bayes) risk rate and an insurer-ish priority:\n",
    "#   expected_incidents_proxy = EB_rate_per_trip * exposure_trips\n",
    "# This avoids tiny-station outliers and produces a \"risk × exposure\" ranking.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "MIN_TRIPS_FOR_SIGNAL = 5000\n",
    "EB_M_PRIOR = 20000.0\n",
    "SCALE = 100000.0\n",
    "\n",
    "if df_score is None:\n",
    "    print(\"No scorecard loaded (df_score is None). Skipping EB enrichment.\")\n",
    "else:\n",
    "    s = df_score.copy()\n",
    "\n",
    "    # Normalize column names\n",
    "    if \"station_id\" not in s.columns and \"start_station_id\" in s.columns:\n",
    "        s = s.rename(columns={\"start_station_id\": \"station_id\"})\n",
    "    if \"station_name\" not in s.columns and \"start_station_name\" in s.columns:\n",
    "        s = s.rename(columns={\"start_station_name\": \"station_name\"})\n",
    "    if \"exposure_trips\" not in s.columns and \"trips\" in s.columns:\n",
    "        s = s.rename(columns={\"trips\": \"exposure_trips\"})\n",
    "\n",
    "    # Ensure mode exists\n",
    "    if \"mode\" not in s.columns:\n",
    "        s[\"mode\"] = mode\n",
    "\n",
    "    # Ensure numerics\n",
    "    for c in [\"exposure_trips\", \"crash_count\", \"risk_rate_per_100k_trips\"]:\n",
    "        if c in s.columns:\n",
    "            s[c] = pd.to_numeric(s[c], errors=\"coerce\")\n",
    "\n",
    "    # If crash_count missing, we cannot do EB\n",
    "    if \"crash_count\" not in s.columns:\n",
    "        print(\"Scorecard missing crash_count; EB enrichment skipped.\")\n",
    "    else:\n",
    "        def eb_per_trip(k: pd.Series, e: pd.Series, m_prior: float) -> pd.Series:\n",
    "            e_sum = float(e.sum())\n",
    "            r0 = float(k.sum() / e_sum) if e_sum > 0 else 0.0\n",
    "            return (k + r0 * m_prior) / (e + m_prior)\n",
    "\n",
    "        out_parts = []\n",
    "        for m, g in s.groupby(\"mode\", dropna=False):\n",
    "            g = g.copy()\n",
    "\n",
    "            g[\"exposure_trips\"] = g[\"exposure_trips\"].fillna(0).astype(float)\n",
    "            g[\"crash_count\"] = g[\"crash_count\"].fillna(0).astype(float)\n",
    "\n",
    "            stable = g[\"exposure_trips\"] >= float(MIN_TRIPS_FOR_SIGNAL)\n",
    "\n",
    "            # Determine if proxy has usable signal in this mode\n",
    "            if \"risk_rate_per_100k_trips\" in g.columns and g[\"risk_rate_per_100k_trips\"].notna().any():\n",
    "                risk_has_signal = (g.loc[stable, \"risk_rate_per_100k_trips\"].nunique(dropna=True) > 1) if stable.any() else False\n",
    "            else:\n",
    "                risk_has_signal = (g.loc[stable, \"crash_count\"].nunique(dropna=True) > 1) if stable.any() else False\n",
    "\n",
    "            g[\"risk_proxy_available\"] = bool(risk_has_signal)\n",
    "\n",
    "            # Exposure percentile (always)\n",
    "            g[\"exposure_index_pct\"] = g[\"exposure_trips\"].rank(pct=True, method=\"average\") * 100.0 if g[\"exposure_trips\"].nunique() > 1 else 50.0\n",
    "\n",
    "            if not risk_has_signal:\n",
    "                # JC-style: exposure only\n",
    "                g[\"eb_risk_rate_per_100k\"] = np.nan\n",
    "                g[\"expected_incidents_proxy\"] = np.nan\n",
    "                g[\"eb_priority_score\"] = g[\"exposure_index_pct\"]  # 0..100\n",
    "                g[\"scoring_strategy\"] = \"exposure_only_no_risk_signal\"\n",
    "                out_parts.append(g)\n",
    "                continue\n",
    "\n",
    "            # EB rate and expected incidents proxy\n",
    "            eb_rate = eb_per_trip(g[\"crash_count\"], g[\"exposure_trips\"], float(EB_M_PRIOR))\n",
    "            g[\"eb_risk_rate_per_100k\"] = eb_rate * SCALE\n",
    "            g[\"expected_incidents_proxy\"] = eb_rate * g[\"exposure_trips\"]\n",
    "\n",
    "            # Priority = percentile of expected incidents (0..100)\n",
    "            g[\"eb_priority_score\"] = g[\"expected_incidents_proxy\"].rank(pct=True, method=\"average\") * 100.0 if g[\"expected_incidents_proxy\"].nunique() > 1 else 50.0\n",
    "            g[\"scoring_strategy\"] = f\"eb_expected_incidents_mprior{int(EB_M_PRIOR)}\"\n",
    "\n",
    "            out_parts.append(g)\n",
    "\n",
    "        s2 = pd.concat(out_parts, ignore_index=True)\n",
    "\n",
    "        # Simple action tags\n",
    "        s2[\"eb_prevention_hotspot\"] = (s2[\"exposure_index_pct\"] >= 80.0) & (s2[\"eb_priority_score\"] >= 80.0)\n",
    "        s2[\"eb_product_hotspot\"] = (s2[\"exposure_index_pct\"] >= 80.0)\n",
    "        s2[\"eb_acquisition_hotspot\"] = (s2[\"exposure_index_pct\"] >= 70.0) & (s2.get(\"eb_risk_rate_per_100k\").fillna(0) <= s2.get(\"eb_risk_rate_per_100k\").median())\n",
    "\n",
    "        df_score = s2  # overwrite for downstream cells\n",
    "\n",
    "        print(\"EB enrichment complete. Proxy available share by mode:\")\n",
    "        display(df_score.groupby(\"mode\")[\"risk_proxy_available\"].mean().to_frame(\"proxy_available_share\"))\n",
    "\n",
    "        display(\n",
    "            df_score.sort_values(\"eb_priority_score\", ascending=False)\n",
    "                    .head(15)[[\n",
    "                        \"mode\",\"station_id\",\"station_name\",\n",
    "                        \"exposure_trips\",\"crash_count\",\"risk_proxy_available\",\n",
    "                        \"eb_risk_rate_per_100k\",\"expected_incidents_proxy\",\"eb_priority_score\",\n",
    "                        \"scoring_strategy\"\n",
    "                    ]]\n",
    "        )\n",
    "\n",
    "        out_path = RUN_DIR / \"axa_partner_scorecard_500m_with_eb.csv\"\n",
    "        df_score.to_csv(out_path, index=False)\n",
    "        print(\"Wrote:\", out_path)\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a162d7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHERE + WHEN + WHAT (turn the outputs into decision-ready assets)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "if df_score is None or df_windows is None:\n",
    "    print(\"AXA assets not available for this run; skipping WHERE/WHEN/WHAT tables.\")\n",
    "else:\n",
    "    score = df_score.copy()\n",
    "\n",
    "    # Normalize station columns\n",
    "    if \"station_id\" not in score.columns and \"start_station_id\" in score.columns:\n",
    "        score = score.rename(columns={\"start_station_id\": \"station_id\"})\n",
    "    if \"station_name\" not in score.columns and \"start_station_name\" in score.columns:\n",
    "        score = score.rename(columns={\"start_station_name\": \"station_name\"})\n",
    "\n",
    "    # Priority preference: EB first (works for NYC; for JC it becomes exposure-only)\n",
    "    if \"eb_priority_score\" in score.columns:\n",
    "        pri_col = \"eb_priority_score\"\n",
    "    elif \"axa_priority_score\" in score.columns:\n",
    "        pri_col = \"axa_priority_score\"\n",
    "    elif \"exposure_trips\" in score.columns:\n",
    "        pri_col = \"exposure_trips\"\n",
    "    else:\n",
    "        pri_col = None\n",
    "\n",
    "    # Columns to display (de-duplicated)\n",
    "    cols = [\n",
    "        \"mode\",\"station_id\",\"station_name\",\n",
    "        \"exposure_trips\",\"crash_count\",\"risk_proxy_available\",\n",
    "        \"risk_rate_per_100k_trips\",\"risk_rate_ci_low\",\"risk_rate_ci_high\",\n",
    "        \"eb_risk_rate_per_100k\",\"expected_incidents_proxy\",\"eb_priority_score\",\n",
    "        \"scoring_strategy\",\n",
    "        \"eb_prevention_hotspot\",\"eb_product_hotspot\",\"eb_acquisition_hotspot\",\n",
    "    ]\n",
    "    cols = [c for c in cols if c in score.columns]\n",
    "    cols = list(dict.fromkeys(cols))\n",
    "\n",
    "    # WHERE\n",
    "    if pri_col is None:\n",
    "        print(\"No usable priority column found; showing first 20 rows of scorecard.\")\n",
    "        where_top = score.head(20)\n",
    "    else:\n",
    "        # Prefer proxy-covered first (NYC), then rank\n",
    "        if \"risk_proxy_available\" in score.columns:\n",
    "            where_top = score.sort_values([\"risk_proxy_available\", pri_col], ascending=[False, False]).head(20)\n",
    "        else:\n",
    "            where_top = score.sort_values(pri_col, ascending=False).head(20)\n",
    "\n",
    "    print(f\"WHERE to focus — top 20 stations by `{pri_col}`\")\n",
    "    display(where_top[cols] if cols else where_top)\n",
    "\n",
    "    # WHEN\n",
    "    w = df_windows.copy()\n",
    "    for c in [\"trips\", \"index\", \"lift_pct\", \"lift_vs_baseline_pct\"]:\n",
    "        if c in w.columns:\n",
    "            w[c] = pd.to_numeric(w[c], errors=\"coerce\")\n",
    "\n",
    "    if \"lift_vs_baseline_pct\" in w.columns:\n",
    "        w_rank = \"lift_vs_baseline_pct\"\n",
    "    elif \"lift_pct\" in w.columns:\n",
    "        w_rank = \"lift_pct\"\n",
    "    elif \"index\" in w.columns:\n",
    "        w_rank = \"index\"\n",
    "    elif \"trips\" in w.columns:\n",
    "        w_rank = \"trips\"\n",
    "    else:\n",
    "        w_rank = None\n",
    "\n",
    "    when_top = w.sort_values(w_rank, ascending=False).head(20) if w_rank else w.head(20)\n",
    "    print(f\"WHEN to activate — top 20 windows by `{w_rank}`\")\n",
    "    display(when_top)\n",
    "\n",
    "    # WHAT (brief, interview-ready)\n",
    "    print(\"\\nWHAT to propose (one-slide plan):\")\n",
    "    print(\"- NYC: use EB hotspots for prevention pilots + risk-tiered messaging/offer placement.\")\n",
    "    print(\"- JC: exposure-led rollout now; Phase 2 = add JC-local crash proxy for real risk tiering.\")\n",
    "    print(\"- Measure: attach rate, conversion uplift in target windows, and (later) claims frequency once product exists.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdc52d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Quick sanity checks for AXA assets\n",
    "if 'df_score' in globals() and df_score is not None:\n",
    "    print(\"Scorecard columns:\", list(df_score.columns))\n",
    "if 'df_windows' in globals() and df_windows is not None:\n",
    "    print(\"Windows columns:\", list(df_windows.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc904a42",
   "metadata": {},
   "source": [
    "## Interpreting the outputs for this run (plain language)\n",
    "\n",
    "### WHERE (stations)\n",
    "- **Exposure** = how much the station is used (more touchpoints).\n",
    "- **Risk proxy** = how crash-dense the surrounding area is (not “Citi Bike crashes”, but a consistent hazard signal).\n",
    "- Best insurer focus is **High exposure + High risk** stations (largest expected hazard load).\n",
    "- High exposure + low risk stations are **growth/upsell** targets.\n",
    "\n",
    "### WHEN (windows)\n",
    "Use the windows table to choose *activation moments*:\n",
    "- Weekday peaks are typically commuting hours → best for acquisition + add-on messaging.\n",
    "- Weekend peaks are leisure → different tone (safety nudges + leisure bundles).\n",
    "\n",
    "### WHAT (actions)\n",
    "- **Product**: in-app add-on prompt near high exposure stations during commuter peaks.\n",
    "- **Prevention**: safety nudges and partner interventions where exposure + risk proxy are both high.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b86ae6",
   "metadata": {},
   "source": [
    "## Where are the generated plots and tables?\n",
    "\n",
    "Figures are saved under:\n",
    "\n",
    "- `reports/<RUN_TAG>/figures/`\n",
    "\n",
    "Key files:\n",
    "- `09_exposure_vs_risk_zones.png`\n",
    "- `station_zones.csv`\n",
    "- `axa_partner_scorecard_500m.csv`\n",
    "- `axa_target_windows.csv`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6f9fa6",
   "metadata": {},
   "source": [
    "## Statistical correctness and limitations (business-safe)\n",
    "\n",
    "- The “risk proxy” is **proximity to reported crashes**, not confirmed Citi Bike incidents.\n",
    "- Filtering (`MIN_TRIPS_FOR_RISK`) prevents tiny-sample stations from producing misleadingly extreme rates.\n",
    "- For partial-month windows (e.g. Jan–Mar), interpret station rankings as “within-window priorities”, not annual truth.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "modified_at": "2026-01-03T11:12:10.237164Z",
  "modified_by": "chatgpt"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
